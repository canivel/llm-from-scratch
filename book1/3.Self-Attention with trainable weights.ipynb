{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBdKEhVCfU5G",
    "outputId": "6aa3549c-fe56-40bd-c354-7a6ff0baf007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Self-Attention with Trainable Weights\n\nIn the previous notebook, we computed attention using raw embeddings directly. The problem? **Those embeddings were hand-crafted** — in reality, the model needs to **learn** what to pay attention to.\n\nThis notebook introduces **trainable weight matrices** (Q, K, V) that transform inputs before computing attention:\n\n```\nQuery = input @ W_query    \"What am I looking for?\"\nKey   = input @ W_key      \"What do I contain?\"  \nValue = input @ W_value    \"What do I provide?\"\n```\n\n---\n\n## Understanding Q, K, V in Simple Terms\n\n### The Library Analogy\n\nImagine you're in a library looking for information about \"how to bake a cake.\"\n\n| Concept | Library Analogy | What It Does |\n|---------|-----------------|--------------|\n| **Query (Q)** | Your question: \"I need cake recipes\" | What you're **searching for** |\n| **Key (K)** | Book titles/labels on the shelf | What each book **advertises** it contains |\n| **Value (V)** | The actual content inside books | The **real information** you'll get |\n\n#### How it works step-by-step:\n\n1. **You form a Query** → \"I want cake recipes\"\n2. **You compare your Query against all Keys** → You scan book titles\n3. **High match = high attention** → \"The Art of Baking\" matches well!\n4. **You retrieve Values based on matches** → You read from the matching books\n\n---\n\n### Why Separate Q, K, V? Why Not Just Use the Input Directly?\n\nThis is the key insight! Let's think about the words in our sentence:\n\n**Sentence:** `\"Your journey starts with one step\"`\n\nConsider the word **\"journey\"** (x²):\n- When \"journey\" is **asking a question** (Query): \"What words relate to me? What starts? With what?\" → needs to find connections\n- When \"journey\" is **being searched** (Key): \"I am a noun, an abstract concept, the main subject\" → needs to advertise what it is\n- When \"journey\" **provides information** (Value): \"Here's my semantic meaning to contribute\" → the actual content to pass forward\n\n**These are THREE DIFFERENT ROLES for the same word!**\n\n---\n\n### Visual Example with Our Sentence\n\n```\nInput word: \"journey\" (x²) = [0.55, 0.87, 0.66]\n\n                    ┌──────────────────┐\n                    │    \"journey\"     │\n                    │  [0.55, 0.87,    │\n                    │       0.66]      │\n                    └────────┬─────────┘\n                             │\n             ┌───────────────┼───────────────┐\n             │               │               │\n             ▼               ▼               ▼\n        ┌─────────┐    ┌─────────┐    ┌─────────┐\n        │× W_query│    │× W_key  │    │× W_value│\n        └────┬────┘    └────┬────┘    └────┬────┘\n             │               │               │\n             ▼               ▼               ▼\n        ┌─────────┐    ┌─────────┐    ┌─────────┐\n        │Query    │    │ Key     │    │Value    │\n        │\"What am │    │\"What I  │    │\"What I  │\n        │I looking│    │contain\" │    │provide\" │\n        │  for?\"  │    │         │    │         │\n        │         │    │         │    │         │\n        │[0.43,   │    │[0.44,   │    │[0.40,   │\n        │ 1.46]   │    │ 1.14]   │    │ 1.00]   │\n        └─────────┘    └─────────┘    └─────────┘\n\nThen \"journey\"'s Query compares against ALL Keys:\n\n        Query₂ @ Keys.T = Attention Scores\n        \n        \"journey\" asks: \"How relevant is each word to me?\"\n        \n        Your:     1.27  ←─┐\n        journey:  1.85  ←─┤\n        starts:   1.81  ←─┼── These scores show \"journey\" \n        with:     1.08  ←─┤   attends most to itself and \"starts\"\n        one:      0.56  ←─┤\n        step:     1.54  ←─┘\n```\n\n---\n\n### The Math (Simplified)\n\n```python\n# Each word gets transformed THREE different ways:\n\nQ = input @ W_query   # Shape: [6, 2] - all 6 words get queries\nK = input @ W_key     # Shape: [6, 2] - all 6 words get keys\nV = input @ W_value   # Shape: [6, 2] - all 6 words get values\n\n# Then attention happens:\nattention_scores = Q @ K.T      # \"How much does each query match each key?\"\nattention_weights = softmax(attention_scores)  # Normalize to probabilities\noutput = attention_weights @ V  # Weighted sum of values\n```\n\n---\n\n### Why This is Powerful (The Learning Part)\n\n| Without Q,K,V | With Q,K,V |\n|---------------|------------|\n| Fixed attention based on raw similarity | **Learnable** attention patterns |\n| \"journey\" always looks the same when asking or answering | \"journey\" can learn different roles |\n| Can't adapt to task | Model learns what's important through training |\n\n**The W matrices are LEARNED during training!** The model discovers:\n- What features to look for (W_query)\n- What features to advertise (W_key)\n- What features to pass forward (W_value)\n\n---\n\n### Super Simple Summary\n\n| | Query | Key | Value |\n|---|-------|-----|-------|\n| **Question** | \"What do I need?\" | \"What do I have?\" | \"Here's my content\" |\n| **Search engine** | Your search terms | Webpage titles | Webpage content |\n| **Our example** | \"journey\" asks: \"who relates to me?\" | Each word's label: \"I'm a verb/noun/etc\" | Each word's actual meaning |\n\nThe genius is: **the same word plays all three roles**, but with **different learned transformations** for each role!\n\n---\n\n**Why this matters:**\n- The model can **learn** different representations for querying vs. being queried\n- Attention patterns become **trainable** via backpropagation\n- This is how attention works in real transformers like GPT!\n\n## Install Dependencies"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Input Embeddings\n",
    "\n",
    "We use the same hand-crafted embeddings from the previous notebook. Remember: in a real model, these would come from a learned embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dimensions\n",
    "\n",
    "- `x_2`: The \"journey\" token embedding (our query token)\n",
    "- `d_in = 3`: Input dimension (size of each embedding)\n",
    "- `d_out = 2`: Output dimension (size of Q, K, V vectors)\n",
    "\n",
    "**Note:** `d_out` can differ from `d_in`! This allows the model to project embeddings into a different dimensional space for attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "print(x_2)\n",
    "print(d_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Weight Matrices (W_query, W_key, W_value)\n",
    "\n",
    "These are the **trainable parameters** that transform inputs into queries, keys, and values.\n",
    "\n",
    "Each matrix has shape `[d_in, d_out]` = `[3, 2]`:\n",
    "- Takes a 3-dimensional input embedding\n",
    "- Produces a 2-dimensional Q, K, or V vector\n",
    "\n",
    "**Key insight:** These matrices start with **random values** (unlike our hand-crafted embeddings). During training, backpropagation will adjust these weights so the model learns **what to pay attention to**.\n",
    "\n",
    "`requires_grad=False` is set here just for demonstration — in real training, we'd want gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "print(W_query)\n",
    "print(W_key)\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Query, Key, Value for One Token\n",
    "\n",
    "Transform \"journey\" (`x_2`) through each weight matrix:\n",
    "\n",
    "```\n",
    "x_2 [1×3] @ W_query [3×2] = query_2 [1×2]\n",
    "x_2 [1×3] @ W_key   [3×2] = key_2   [1×2]  \n",
    "x_2 [1×3] @ W_value [3×2] = value_2 [1×2]\n",
    "```\n",
    "\n",
    "**Intuition:**\n",
    "- `query_2`: \"What is 'journey' looking for?\"\n",
    "- `key_2`: \"What does 'journey' contain that others might want?\"\n",
    "- `value_2`: \"What information does 'journey' provide when attended to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Keys and Values for ALL Tokens\n",
    "\n",
    "We need keys and values for every token (so \"journey\" can compare against all of them).\n",
    "\n",
    "```\n",
    "inputs [6×3] @ W_key   [3×2] = keys   [6×2]\n",
    "inputs [6×3] @ W_value [3×2] = values [6×2]\n",
    "```\n",
    "\n",
    "Each token now has its own 2-dimensional key and value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Attention Scores\n",
    "\n",
    "Now we compute how much \"journey\" should attend to each token using the **dot product between query and keys**.\n",
    "\n",
    "First, a single attention score (journey attending to itself):\n",
    "```\n",
    "attn_score_22 = query_2 · key_2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute attention scores for \"journey\" against ALL tokens at once:\n",
    "\n",
    "```\n",
    "query_2 [1×2] @ keys.T [2×6] = attn_scores_2 [1×6]\n",
    "```\n",
    "\n",
    "**Compare to previous notebook:** We're doing the same dot product operation, but now using **transformed** Q and K vectors instead of raw embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "**Problem:** When `d_k` (key dimension) is large, dot products can become very large, pushing softmax into regions with tiny gradients.\n",
    "\n",
    "**Solution:** Scale by `√d_k` before applying softmax:\n",
    "\n",
    "```\n",
    "attn_weights = softmax(attn_scores / √d_k)\n",
    "```\n",
    "\n",
    "Here `d_k = 2`, so we divide by `√2 ≈ 1.414`. This keeps the variance of the dot products stable regardless of dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Context Vector\n",
    "\n",
    "Same as before: weighted sum of values using the attention weights.\n",
    "\n",
    "```\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "```\n",
    "\n",
    "The output is now 2-dimensional (matching `d_out`) instead of 3-dimensional (the original `d_in`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap It All in a PyTorch Module (v1)\n",
    "\n",
    "Let's package everything into a reusable `nn.Module` class.\n",
    "\n",
    "**SelfAttention_v1** uses `nn.Parameter` with raw tensors:\n",
    "- Weights are initialized with `torch.rand()` (uniform 0-1)\n",
    "- The `@` operator does matrix multiplication\n",
    "- `grad_fn=<MmBackward0>` shows PyTorch is tracking gradients for backprop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Version with nn.Linear (v2)\n",
    "\n",
    "**SelfAttention_v2** uses `nn.Linear` layers instead of raw `nn.Parameter`:\n",
    "\n",
    "**Advantages of `nn.Linear`:**\n",
    "- Better weight initialization (Kaiming/Xavier by default)\n",
    "- Optional bias term (`qkv_bias` parameter)\n",
    "- Cleaner syntax: `self.W_key(x)` instead of `x @ self.W_key`\n",
    "- More consistent with PyTorch conventions\n",
    "\n",
    "**Note:** The outputs are different because:\n",
    "1. Different random seed (789 vs 123)\n",
    "2. `nn.Linear` uses a different weight initialization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}