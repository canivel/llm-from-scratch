{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook_intro",
   "metadata": {},
   "source": [
    "# Pre-Training a GPT Model from Scratch\n",
    "\n",
    "This notebook walks through the complete process of **pre-training** a GPT-style language model. Pre-training is the foundational step where the model learns general language patterns from a large corpus of text.\n",
    "\n",
    "## What is Pre-Training?\n",
    "\n",
    "Pre-training teaches a model to predict the next token in a sequence. Given text like:\n",
    "\n",
    "```\n",
    "\"The cat sat on the\" \u2192 predict \"mat\"\n",
    "```\n",
    "\n",
    "Through millions of these predictions, the model learns:\n",
    "- Grammar and syntax\n",
    "- Word relationships and semantics  \n",
    "- Facts and knowledge from the training data\n",
    "- Reasoning patterns\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "| Part | Topic |\n",
    "|------|-------|\n",
    "| 1 | **Model Architecture** - Build all GPT components (attention, feed-forward, transformer blocks) |\n",
    "| 2 | **Text Generation** - See what an untrained model produces |\n",
    "| 3 | **Loss Function** - Understand cross-entropy loss for next-token prediction |\n",
    "| 4 | **Data Preparation** - Create training batches from raw text |\n",
    "| 5 | **Training Loop** - Put it all together to train the model |\n",
    "| 6 | **Decoding Strategies** - Temperature and Top-k sampling |\n",
    "| 7 | **Saving Models** - Persist and load trained weights |\n",
    "| 8 | **Loading GPT-2** - Use pre-trained weights from OpenAI |\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Pre-training is **expensive** (GPT-3 cost ~$4.6M to train!) but creates a foundation that can be fine-tuned for many tasks. Understanding this process helps you:\n",
    "- Debug training issues\n",
    "- Make informed architecture decisions\n",
    "- Understand model capabilities and limitations\n",
    "\n",
    "---\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required packages:\n",
    "- **torch**: PyTorch deep learning framework\n",
    "- **tiktoken**: OpenAI's fast BPE tokenizer\n",
    "- **transformers**: Hugging Face library (for comparison/utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d22ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device_detection_intro",
   "metadata": {},
   "source": [
    "## Setup: Detecting Hardware Acceleration\n",
    "\n",
    "Modern deep learning benefits enormously from **hardware acceleration**. This function automatically detects the best available device:\n",
    "\n",
    "| Device | Speed | Availability |\n",
    "|--------|-------|--------------|\n",
    "| **TPU** | Fastest | Google Colab Pro, Cloud TPU |\n",
    "| **CUDA GPU** | Very Fast | NVIDIA graphics cards |\n",
    "| **Apple MPS** | Fast | Apple Silicon Macs (M1/M2/M3) |\n",
    "| **CPU** | Slowest | Always available |\n",
    "\n",
    "The function checks in order of preference and returns the best available option. For this notebook:\n",
    "- **GPU/TPU**: Training takes ~5-10 minutes\n",
    "- **CPU**: Training takes ~30-60 minutes (but still works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26c91ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 GPU detected: NVIDIA A100-SXM4-40GB\n",
      "  - CUDA Version: 12.6\n",
      "  - GPU Memory: 42.47 GB\n",
      "\n",
      "\u2192 Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Device Detection: CPU, GPU (CUDA), or TPU\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Detect and return the best available device.\"\"\"\n",
    "    \n",
    "    # Check for TPU (Google Colab / Cloud TPU)\n",
    "    try:\n",
    "        import torch_xla\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        device = xm.xla_device()\n",
    "        device_name = \"TPU\"\n",
    "        print(f\"\u2713 TPU detected!\")\n",
    "        return device, device_name\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Check for CUDA GPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"\u2713 GPU detected: {device_name}\")\n",
    "        print(f\"  - CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"  - GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        return device, device_name\n",
    "    \n",
    "    # Check for Apple Silicon MPS\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        device_name = \"Apple Silicon (MPS)\"\n",
    "        print(f\"\u2713 Apple Silicon detected: {device_name}\")\n",
    "        return device, device_name\n",
    "    \n",
    "    # Fallback to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    print(f\"\u26a0 No GPU/TPU detected. Using CPU.\")\n",
    "    print(f\"  - Training will be slower on CPU\")\n",
    "    return device, device_name\n",
    "\n",
    "# Detect device\n",
    "device, device_name = get_device()\n",
    "print(f\"\\n\u2192 Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lvf4rwpewk",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Building the Model Architecture\n",
    "\n",
    "First, we need to assemble all the building blocks of our GPT model. We've covered these in previous notebooks, so here we'll import/define them and focus on how they work together.\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "The **heart of the transformer**. Multi-head attention allows the model to:\n",
    "- Look at different positions in the sequence simultaneously\n",
    "- Learn different types of relationships (syntactic, semantic, etc.) via multiple \"heads\"\n",
    "- Use **causal masking** to prevent looking at future tokens (crucial for language modeling!)\n",
    "\n",
    "### How Multi-Head Attention Works:\n",
    "\n",
    "```\n",
    "Input: [batch, seq_len, d_model]\n",
    "         \u2502\n",
    "         \u251c\u2500\u2500\u2192 Q = input @ W_query \u2500\u2500\u2510\n",
    "         \u251c\u2500\u2500\u2192 K = input @ W_key   \u2500\u2500\u253c\u2500\u2500\u2192 Split into num_heads\n",
    "         \u2514\u2500\u2500\u2192 V = input @ W_value \u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502     For each head:            \u2502\n",
    "                    \u2502  1. attention = Q @ K.T       \u2502\n",
    "                    \u2502  2. mask future positions     \u2502\n",
    "                    \u2502  3. scale by \u221ad_k             \u2502\n",
    "                    \u2502  4. softmax \u2192 weights         \u2502\n",
    "                    \u2502  5. output = weights @ V      \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                            Concatenate all heads\n",
    "                                    \u2502\n",
    "                            Output projection\n",
    "                                    \u2502\n",
    "Output: [batch, seq_len, d_model]\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "- `d_in`: Input dimension (embedding size)\n",
    "- `d_out`: Output dimension (usually same as d_in)\n",
    "- `num_heads`: Number of attention heads (d_out must be divisible by this)\n",
    "- `context_length`: Maximum sequence length (for creating the causal mask)\n",
    "- `dropout`: Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb09ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8ubeyg4p",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "**Layer Normalization** stabilizes training by normalizing activations across the feature dimension.\n",
    "\n",
    "### Why We Need It:\n",
    "\n",
    "During training, the distribution of layer inputs can shift dramatically (called \"internal covariate shift\"). This makes training unstable and slow. Layer normalization fixes this by:\n",
    "\n",
    "1. **Normalizing** each sample independently across features\n",
    "2. **Scaling and shifting** with learnable parameters (so the model can undo normalization if needed)\n",
    "\n",
    "### The Math:\n",
    "\n",
    "```\n",
    "For each token position:\n",
    "    mean = average of all features\n",
    "    var  = variance of all features\n",
    "    \n",
    "    normalized = (x - mean) / \u221a(var + \u03b5)\n",
    "    output = scale * normalized + shift\n",
    "```\n",
    "\n",
    "### Why Layer Norm (not Batch Norm)?\n",
    "\n",
    "| Batch Norm | Layer Norm |\n",
    "|------------|------------|\n",
    "| Normalizes across batch | Normalizes across features |\n",
    "| Depends on batch size | Independent of batch size |\n",
    "| Different behavior train/eval | Same behavior always |\n",
    "| Bad for sequences | Great for sequences |\n",
    "\n",
    "**GPT uses Layer Norm** because it works independently for each token, regardless of batch size or sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8861d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oggglia1mjl",
   "metadata": {},
   "source": [
    "## GELU Activation Function\n",
    "\n",
    "**GELU** (Gaussian Error Linear Unit) is the activation function used in GPT and most modern transformers.\n",
    "\n",
    "### Why Not ReLU?\n",
    "\n",
    "ReLU is simple (`max(0, x)`) but has a problem: **dead neurons**. Once a neuron outputs 0, it may never recover during training.\n",
    "\n",
    "### How GELU Works:\n",
    "\n",
    "GELU is a **smooth approximation** that:\n",
    "- Allows small negative values through (unlike ReLU which blocks all negatives)\n",
    "- Is differentiable everywhere (unlike ReLU's sharp corner at 0)\n",
    "- Acts like a \"soft gate\" based on the input's value\n",
    "\n",
    "```\n",
    "GELU(x) \u2248 0.5 * x * (1 + tanh(\u221a(2/\u03c0) * (x + 0.044715 * x\u00b3)))\n",
    "```\n",
    "\n",
    "### Why GELU is Better for Transformers:\n",
    "- Smoother gradients \u2192 more stable training\n",
    "- No dead neurons \u2192 better gradient flow\n",
    "- Empirically works better for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16164a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjvjx8ogw5",
   "metadata": {},
   "source": [
    "## Feed-Forward Network (FFN)\n",
    "\n",
    "The **Feed-Forward Network** is the \"thinking\" part of each transformer block. While attention figures out *what to look at*, the FFN processes *what to do with that information*.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Input [batch, seq, 768]\n",
    "        \u2502\n",
    "        \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Linear (768\u21923072)\u2502  \u2190 Expand to 4x size\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "          \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502      GELU         \u2502  \u2190 Non-linearity\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "          \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Linear (3072\u2192768)\u2502  \u2190 Project back down\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "          \u25bc\n",
    "Output [batch, seq, 768]\n",
    "```\n",
    "\n",
    "### Why 4x Expansion?\n",
    "\n",
    "The FFN temporarily expands the dimension by **4x** (768 \u2192 3072). This gives the network more \"room to think\":\n",
    "\n",
    "1. **More parameters** = more capacity to learn complex patterns\n",
    "2. **Bottleneck design** = forces compression of information\n",
    "3. **Empirically effective** = this ratio works well in practice\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "The FFN processes each token position **independently**. Unlike attention (which mixes information across positions), the FFN applies the same transformation to each token separately. This is why it's also called a \"position-wise\" feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbc34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mb5qeio24i",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "The **Transformer Block** combines attention and feed-forward layers with **residual connections** and **layer normalization**. This is the repeating unit that gets stacked to create deep models.\n",
    "\n",
    "### Architecture (Pre-Norm variant used in GPT-2/3):\n",
    "\n",
    "```\n",
    "        Input\n",
    "          \u2502\n",
    "          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u2502                     \u2502 (residual/skip connection)\n",
    "          \u25bc                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "    \u2502 LayerNorm \u2502               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "          \u2502                     \u2502\n",
    "          \u25bc                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "    \u2502 Attention \u2502               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "          \u2502                     \u2502\n",
    "          \u25bc                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "    \u2502  Dropout  \u2502               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "          \u2502                     \u2502\n",
    "          \u25bc                     \u2502\n",
    "        (+)\u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u2502                     \u2502 (residual/skip connection)\n",
    "          \u25bc                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "    \u2502 LayerNorm \u2502               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "          \u2502                     \u2502\n",
    "          \u25bc                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "    \u2502    FFN    \u2502               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "          \u2502                     \u2502\n",
    "          \u25bc                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "    \u2502  Dropout  \u2502               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "          \u2502                     \u2502\n",
    "          \u25bc                     \u2502\n",
    "        (+)\u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "        Output\n",
    "```\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "Residual (skip) connections are **critical** for training deep networks:\n",
    "\n",
    "1. **Gradient flow**: Gradients can flow directly through skip connections, avoiding vanishing gradients\n",
    "2. **Identity mapping**: The network can easily learn to \"do nothing\" if that's optimal\n",
    "3. **Incremental learning**: Each layer learns a \"delta\" on top of existing representations\n",
    "\n",
    "### Pre-Norm vs Post-Norm\n",
    "\n",
    "GPT-2/3 use **Pre-Norm** (normalize before attention/FFN), which:\n",
    "- Is more stable during training\n",
    "- Allows for larger learning rates\n",
    "- Enables training very deep models without warmup tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477a8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5a1p81o7e",
   "metadata": {},
   "source": [
    "## GPT Configuration\n",
    "\n",
    "Before building the model, we define its **hyperparameters**. These control the model's size and capacity.\n",
    "\n",
    "### GPT-2 124M Configuration:\n",
    "\n",
    "| Parameter | Value | Meaning |\n",
    "|-----------|-------|---------|\n",
    "| `vocab_size` | 50,257 | Number of unique tokens (BPE vocabulary) |\n",
    "| `context_length` | 256 | Maximum sequence length (reduced from 1024 for speed) |\n",
    "| `emb_dim` | 768 | Size of token embeddings |\n",
    "| `n_heads` | 12 | Number of attention heads |\n",
    "| `n_layers` | 12 | Number of transformer blocks |\n",
    "| `drop_rate` | 0.1 | Dropout probability (10%) |\n",
    "| `qkv_bias` | False | No bias in Q, K, V projections |\n",
    "\n",
    "### Scaling Laws:\n",
    "\n",
    "The model's capacity roughly scales with:\n",
    "- **Width** (`emb_dim`): More features per token\n",
    "- **Depth** (`n_layers`): More processing steps\n",
    "- **Heads** (`n_heads`): More parallel attention patterns\n",
    "\n",
    "GPT-2 sizes:\n",
    "- **124M**: 12 layers, 768 dim, 12 heads (what we're using)\n",
    "- **355M**: 24 layers, 1024 dim, 16 heads\n",
    "- **774M**: 36 layers, 1280 dim, 20 heads\n",
    "- **1.5B**: 48 layers, 1600 dim, 25 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24d0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8h3x3h4oyk",
   "metadata": {},
   "source": [
    "## The Complete GPT Model\n",
    "\n",
    "Now we assemble all components into the full **GPTModel** class.\n",
    "\n",
    "### Architecture Overview:\n",
    "\n",
    "```\n",
    "Token IDs [batch, seq_len]\n",
    "          \u2502\n",
    "          \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Token Embedding    \u2502  Look up vectors for each token\n",
    "\u2502  [vocab \u2192 emb_dim]  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Position Embedding  \u2502  Add position information\n",
    "\u2502 [seq_len \u2192 emb_dim] \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502     Dropout         \u2502  Regularization\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Transformer Block  \u2502 \u2500\u2510\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "           \u2502             \u2502\n",
    "           \u25bc             \u2502 \u00d7 12 layers\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  Transformer Block  \u2502  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "           \u2502             \u2502\n",
    "          ...           \u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502    Final LayerNorm  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502    Output Head      \u2502  Project to vocabulary size\n",
    "\u2502 [emb_dim \u2192 vocab]   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502\n",
    "           \u25bc\n",
    "    Logits [batch, seq_len, vocab_size]\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Token + Position Embeddings**: The model needs both *what* token and *where* it is\n",
    "2. **Stacked Transformer Blocks**: 12 identical blocks, each refining the representations\n",
    "3. **Output Head**: Maps final representations back to vocabulary probabilities\n",
    "4. **No bias in output**: `bias=False` in the final Linear layer (GPT-2 convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e3077ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6q5xqgprjf",
   "metadata": {},
   "source": [
    "## Instantiate the Model\n",
    "\n",
    "Let's create our GPT model and examine its structure.\n",
    "\n",
    "### What Happens Here:\n",
    "\n",
    "1. **Set random seed**: Ensures reproducible weight initialization\n",
    "2. **Create model**: Instantiates all layers with random weights\n",
    "3. **Set to eval mode**: Disables dropout (important for consistent outputs during testing)\n",
    "\n",
    "### Model Size:\n",
    "\n",
    "With our configuration, the model has approximately **124 million parameters**:\n",
    "- Token embeddings: 50,257 \u00d7 768 = ~38.6M\n",
    "- Position embeddings: 256 \u00d7 768 = ~0.2M  \n",
    "- Each transformer block: ~7M parameters\n",
    "- 12 blocks: ~84M\n",
    "- Output head: 768 \u00d7 50,257 = ~38.6M (but shares weights with token embeddings in original GPT-2)\n",
    "\n",
    "**Note**: Our implementation doesn't share weights between token embeddings and output head, so we have slightly more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b0455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u88tlchooe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Text Generation (Before Training)\n",
    "\n",
    "Before training, let's see what our randomly-initialized model produces. Spoiler: it will be **complete gibberish**!\n",
    "\n",
    "## Text Generation Process\n",
    "\n",
    "The model generates text **one token at a time** using this loop:\n",
    "\n",
    "```\n",
    "1. Input: \"Every effort moves you\"\n",
    "2. Model predicts probability distribution over ALL 50,257 tokens\n",
    "3. Select the most likely token (argmax) or sample from distribution\n",
    "4. Append new token to input\n",
    "5. Repeat steps 2-4 for desired length\n",
    "```\n",
    "\n",
    "### Helper Functions:\n",
    "\n",
    "- **`text_to_token_ids`**: Converts text \u2192 token IDs using GPT-2's BPE tokenizer\n",
    "- **`token_ids_to_text`**: Converts token IDs \u2192 text\n",
    "- **`generate_text_simple`**: The generation loop (greedy decoding with argmax)\n",
    "\n",
    "### Why the Output is Garbage:\n",
    "\n",
    "Our model has **random weights**. It hasn't learned:\n",
    "- What words mean\n",
    "- Grammar rules\n",
    "- Which tokens typically follow others\n",
    "\n",
    "The output will be essentially random tokens. This demonstrates why **pre-training is essential**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb0f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn\u0645 refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def generate_text_simple(model, idx,\n",
    "                         max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qj9jyhq1wi",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Understanding the Loss Function\n",
    "\n",
    "Now we dive into the **most important concept for training**: the loss function.\n",
    "\n",
    "## What is Loss?\n",
    "\n",
    "**Loss** measures how wrong our model's predictions are. For language models:\n",
    "- Lower loss = better predictions\n",
    "- Higher loss = worse predictions\n",
    "\n",
    "## Cross-Entropy Loss for Language Modeling\n",
    "\n",
    "For next-token prediction, we use **cross-entropy loss**:\n",
    "\n",
    "```\n",
    "Loss = -log(probability of correct token)\n",
    "```\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "If the model predicts:\n",
    "- Correct token with 90% probability \u2192 Loss = -log(0.9) = 0.105 (low, good!)\n",
    "- Correct token with 1% probability \u2192 Loss = -log(0.01) = 4.6 (high, bad!)\n",
    "- Correct token with 0.01% probability \u2192 Loss = -log(0.0001) = 9.2 (very high, terrible!)\n",
    "\n",
    "## Setting Up the Example\n",
    "\n",
    "We'll use two short sequences to understand how loss is calculated:\n",
    "\n",
    "```\n",
    "Batch 1: \"every effort moves\" \u2192 target: \"effort moves you\"\n",
    "Batch 2: \"I really like\"      \u2192 target: \"really like chocolate\"\n",
    "```\n",
    "\n",
    "Each input token should predict the **next** token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9804f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1wy4t1tzrg5",
   "metadata": {},
   "source": [
    "### Create Input Tokens\n",
    "\n",
    "These are the **token IDs** for our input sequences. The tokenizer converts words to numbers:\n",
    "- `16833` = \"every\"\n",
    "- `3626` = \" effort\" \n",
    "- `6100` = \" moves\"\n",
    "- `40` = \"I\"\n",
    "- `1107` = \" really\"\n",
    "- `588` = \" like\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb5c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tzsrwh3i3o",
   "metadata": {},
   "source": [
    "### Create Target Tokens\n",
    "\n",
    "The targets are simply the inputs **shifted by one position**. This is the essence of language modeling:\n",
    "\n",
    "```\n",
    "Input:  [every]  [effort] [moves]\n",
    "Target: [effort] [moves]  [you]\n",
    "         \u2191        \u2191        \u2191\n",
    "      predict   predict  predict\n",
    "```\n",
    "\n",
    "Each position learns to predict the **next** token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ebe92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3pfdhoqjz",
   "metadata": {},
   "source": [
    "### Get Model Predictions\n",
    "\n",
    "Run the inputs through the model to get **logits** (raw scores before softmax).\n",
    "\n",
    "The output shape is `[batch_size, seq_len, vocab_size]` = `[2, 3, 50257]`:\n",
    "- 2 sequences in the batch\n",
    "- 3 tokens per sequence\n",
    "- 50,257 scores (one for each possible next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08fbb0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w9j1f0nv3ao",
   "metadata": {},
   "source": [
    "### What Would the Model Predict?\n",
    "\n",
    "Let's see which tokens the model would actually generate (using argmax to pick the highest probability token).\n",
    "\n",
    "**Remember**: The model has random weights, so these predictions will be nonsense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8d7bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j895wxrbkqp",
   "metadata": {},
   "source": [
    "### Compare Predictions vs Targets\n",
    "\n",
    "Let's decode both the targets (what we want) and predictions (what the model says).\n",
    "\n",
    "As expected, the predictions are **completely wrong** - random tokens that don't make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ba4536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdgoffb7c5",
   "metadata": {},
   "source": [
    "### Extract Probabilities for Target Tokens\n",
    "\n",
    "Now let's see **how much probability** the model assigned to the correct tokens.\n",
    "\n",
    "We use fancy indexing to extract just the probabilities for our target tokens:\n",
    "- `probas[0, [0,1,2], targets[0]]` = probabilities at positions 0,1,2 for the target token IDs\n",
    "\n",
    "**Expected**: Very low probabilities (around 1/50,257 \u2248 0.00002) since the model is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d4f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02jjqkjgaqtd",
   "metadata": {},
   "source": [
    "### Convert to Log Probabilities\n",
    "\n",
    "We take the **logarithm** of probabilities because:\n",
    "\n",
    "1. **Numerical stability**: Very small probabilities (like 0.00001) become manageable numbers\n",
    "2. **Easier math**: Products become sums (important for sequences)\n",
    "3. **Convention**: Cross-entropy loss uses log probabilities\n",
    "\n",
    "```\n",
    "prob = 0.00007  \u2192  log(prob) = -9.5\n",
    "prob = 0.00001  \u2192  log(prob) = -11.5\n",
    "```\n",
    "\n",
    "**Note**: More negative = lower probability = worse prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d13e69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qb3r1hjk7a",
   "metadata": {},
   "source": [
    "### Average Log Probability\n",
    "\n",
    "We average the log probabilities across all tokens to get a single number representing overall model performance.\n",
    "\n",
    "This average is **negative** because log probabilities are always \u2264 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "139fe59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zr7q8931jb",
   "metadata": {},
   "source": [
    "### Negative Average Log Probability = Cross-Entropy Loss!\n",
    "\n",
    "We negate the average to get a **positive** loss value:\n",
    "\n",
    "```\n",
    "Cross-Entropy Loss = -mean(log(probability of correct tokens))\n",
    "```\n",
    "\n",
    "This is exactly what `torch.nn.functional.cross_entropy` computes!\n",
    "\n",
    "**Goal of training**: Minimize this loss \u2192 Maximize probability of correct tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62f8d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7nt01tkoeyh",
   "metadata": {},
   "source": [
    "## Using PyTorch's Cross-Entropy Function\n",
    "\n",
    "The manual calculation above was educational, but in practice we use PyTorch's optimized `cross_entropy` function.\n",
    "\n",
    "### The Challenge: Shape Mismatch\n",
    "\n",
    "PyTorch's `cross_entropy` expects:\n",
    "- `input`: `[N, C]` where N = samples, C = classes\n",
    "- `target`: `[N]` \n",
    "\n",
    "But our shapes are:\n",
    "- `logits`: `[batch, seq_len, vocab]` = `[2, 3, 50257]`\n",
    "- `targets`: `[batch, seq_len]` = `[2, 3]`\n",
    "\n",
    "**Solution**: Flatten the batch and sequence dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fbb4570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zw5ndm4ak8",
   "metadata": {},
   "source": [
    "### Flatten for Cross-Entropy\n",
    "\n",
    "```\n",
    "Before flattening:\n",
    "  logits:  [2, 3, 50257]  \u2192  2 batches \u00d7 3 positions \u00d7 50257 vocab\n",
    "  targets: [2, 3]         \u2192  2 batches \u00d7 3 positions\n",
    "\n",
    "After flattening:\n",
    "  logits:  [6, 50257]     \u2192  6 total predictions \u00d7 50257 vocab\n",
    "  targets: [6]            \u2192  6 total targets\n",
    "```\n",
    "\n",
    "Now each of the 6 predictions is treated as an independent classification problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d324b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9mrs9vqbs",
   "metadata": {},
   "source": [
    "### Compute Cross-Entropy Loss\n",
    "\n",
    "PyTorch's `cross_entropy`:\n",
    "1. Applies softmax to logits internally\n",
    "2. Takes log\n",
    "3. Selects values at target indices\n",
    "4. Averages and negates\n",
    "\n",
    "The result matches our manual calculation exactly (10.7940)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab58cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3rdr3u7x2j",
   "metadata": {},
   "source": [
    "## Perplexity: A More Interpretable Metric\n",
    "\n",
    "**Perplexity** is cross-entropy loss converted to a more intuitive scale:\n",
    "\n",
    "```\n",
    "Perplexity = e^(cross_entropy_loss)\n",
    "```\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "Perplexity \u2248 \"How many tokens is the model confused between?\"\n",
    "\n",
    "| Perplexity | Meaning |\n",
    "|------------|---------|\n",
    "| 1 | Perfect! Model is 100% confident and correct |\n",
    "| 10 | Model is \"choosing\" between ~10 equally likely tokens |\n",
    "| 100 | Model is \"choosing\" between ~100 equally likely tokens |\n",
    "| 50,000 | Nearly random guessing (vocab size is 50,257) |\n",
    "\n",
    "### For Our Random Model:\n",
    "\n",
    "Perplexity \u2248 48,726 means the model is essentially guessing randomly among all tokens. After training, we'd expect perplexity in the range of 10-50 for a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff0954ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'pg84.txt'...\n",
      "Successfully downloaded 'pg84.txt'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import requests\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
    "filename = \"pg84.txt\"\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Downloading '{filename}'...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Successfully downloaded '{filename}'\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(f\"'{filename}' already exists, skipping download.\")\n",
    "\n",
    "# Load the text\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nvwgq31ucz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Preparing Training Data\n",
    "\n",
    "Now we need real text data to train on. We'll use **\"Frankenstein\" by Mary Shelley** from Project Gutenberg - it's free, classic literature, and a good size for demonstration.\n",
    "\n",
    "## The Data Pipeline:\n",
    "\n",
    "```\n",
    "Raw Text File\n",
    "     \u2502\n",
    "     \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Tokenize   \u2502  Convert text \u2192 token IDs using BPE\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   Create    \u2502  Sliding window to create input-target pairs\n",
    "\u2502   Dataset   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 DataLoader  \u2502  Batch, shuffle, and iterate efficiently\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "  Training Loop\n",
    "```\n",
    "\n",
    "## Download the Training Text\n",
    "\n",
    "We'll download Frankenstein (~440KB of text, ~106K tokens) - small enough to train quickly but large enough to see real learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09ed44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 438807\n",
      "Tokens: 106361\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wxqwebjaq",
   "metadata": {},
   "source": [
    "### Check Dataset Size\n",
    "\n",
    "Let's see how much data we have:\n",
    "- **Characters**: Raw text length\n",
    "- **Tokens**: After BPE tokenization (typically 3-4 characters per token for English)\n",
    "\n",
    "~106K tokens is small by modern standards (GPT-3 trained on 300B tokens!) but sufficient for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd6e710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bks879rtfh",
   "metadata": {},
   "source": [
    "### Train/Validation Split\n",
    "\n",
    "We split the data into:\n",
    "- **90% Training**: Model learns from this\n",
    "- **10% Validation**: Used to check if model is overfitting\n",
    "\n",
    "**Important**: We split by position, not randomly. This ensures:\n",
    "1. No data leakage between train and validation\n",
    "2. Validation tests the model on \"future\" text it hasn't seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hdeeeomawj",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    \"\"\"Creates input-target pairs using sliding window for next-token prediction.\"\"\"\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \"\"\"Create a DataLoader with GPT-2 BPE tokenization.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wpfpmxk15g",
   "metadata": {},
   "source": [
    "## GPT Dataset: Creating Input-Target Pairs\n",
    "\n",
    "The `GPTDatasetV1` class creates training examples using a **sliding window**:\n",
    "\n",
    "```\n",
    "Text: \"The monster approached the village slowly\"\n",
    "      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "With max_length=4, stride=4:\n",
    "\n",
    "Example 1:\n",
    "  Input:  [The, monster, approached, the]\n",
    "  Target: [monster, approached, the, village]\n",
    "  \n",
    "Example 2:\n",
    "  Input:  [village, slowly, ...]\n",
    "  Target: [slowly, ..., ...]\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Effect |\n",
    "|-----------|--------|\n",
    "| `max_length` | Sequence length (context window size) |\n",
    "| `stride` | How far to move between examples |\n",
    "\n",
    "### Stride Strategies:\n",
    "\n",
    "- **stride = max_length**: No overlap, each token used once per epoch\n",
    "- **stride < max_length**: Overlapping windows, more examples but redundant data\n",
    "- **stride = 1**: Maximum examples, but highly redundant\n",
    "\n",
    "We use `stride = max_length` (256) for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52f89697",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hujsejvwmqo",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "**DataLoaders** handle:\n",
    "- **Batching**: Group examples together for parallel processing\n",
    "- **Shuffling**: Randomize order each epoch (training only)\n",
    "- **Dropping incomplete batches**: Ensures consistent batch sizes\n",
    "\n",
    "Settings:\n",
    "- `batch_size=2`: Small for demonstration (real training uses 32-512)\n",
    "- `shuffle=True` for training, `False` for validation\n",
    "- `drop_last=True` for training to avoid small final batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2298bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ulyhm9iqyt",
   "metadata": {},
   "source": [
    "### Verify Data Loader Output\n",
    "\n",
    "Let's check that our data loaders produce the expected shapes:\n",
    "- Each batch should have shape `[batch_size, seq_length]` = `[2, 256]`\n",
    "- Both inputs (x) and targets (y) have the same shape\n",
    "\n",
    "The training loader should have more batches than the validation loader (90% vs 10% of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2851369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kbhfzhdlx9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Training Infrastructure\n",
    "\n",
    "Before the main training loop, we need helper functions to calculate loss efficiently.\n",
    "\n",
    "## Batch Loss Function\n",
    "\n",
    "`calc_loss_batch` computes the cross-entropy loss for a single batch:\n",
    "\n",
    "1. Move data to the right device (CPU or GPU)\n",
    "2. Run forward pass to get logits\n",
    "3. Flatten and compute cross-entropy loss\n",
    "\n",
    "This is the same loss calculation we did manually earlier, but packaged as a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd852dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atsywcg3dwk",
   "metadata": {},
   "source": [
    "## Dataset Loss Function\n",
    "\n",
    "`calc_loss_loader` computes the average loss over an entire data loader:\n",
    "\n",
    "1. Iterate through batches\n",
    "2. Accumulate losses\n",
    "3. Return the average\n",
    "\n",
    "### Optional `num_batches` Parameter:\n",
    "\n",
    "For large datasets, computing loss on ALL batches is slow. The `num_batches` parameter lets you estimate the loss using only a subset of data - useful for quick progress checks during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0c3dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "co41rtlgkfv",
   "metadata": {},
   "source": [
    "## Device Selection\n",
    "\n",
    "Deep learning benefits enormously from **GPU acceleration**:\n",
    "\n",
    "| Device | Training Speed |\n",
    "|--------|---------------|\n",
    "| CPU | 1x (baseline) |\n",
    "| GPU (CUDA) | 10-100x faster |\n",
    "| TPU | Even faster (specialized) |\n",
    "\n",
    "PyTorch automatically detects if a CUDA-capable GPU is available. If not, it falls back to CPU.\n",
    "\n",
    "**Note**: Training on CPU is fine for learning, but real models require GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed068146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.986330868726109\n",
      "Validation loss: 10.984492619832357\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pv1aqguesck",
   "metadata": {},
   "source": [
    "## Baseline Loss (Before Training)\n",
    "\n",
    "Let's measure the loss on both training and validation sets **before any training**.\n",
    "\n",
    "### What to Expect:\n",
    "\n",
    "For a randomly initialized model with vocabulary size 50,257:\n",
    "- Expected loss \u2248 log(50,257) \u2248 **10.82**\n",
    "- This is the loss you'd get from random guessing\n",
    "\n",
    "If our initial loss is close to 10.82, it confirms the model is starting from random weights (as expected).\n",
    "\n",
    "### Why Check Both Train and Val Loss?\n",
    "\n",
    "- **Training loss**: How well does the model fit the training data?\n",
    "- **Validation loss**: How well does the model generalize to unseen data?\n",
    "\n",
    "If training loss goes down but validation loss goes up \u2192 **overfitting**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_68",
   "metadata": {},
   "source": [
    "## Helper Functions for Training\n",
    "\n",
    "Before we start training, we need two essential helper functions:\n",
    "\n",
    "### 1. `generate_and_print_sample`\n",
    "\n",
    "This function generates sample text during training to **visually monitor progress**:\n",
    "\n",
    "```python\n",
    "1. Set model to eval mode (disables dropout)\n",
    "2. Encode the starting text to token IDs\n",
    "3. Generate new tokens using our simple generation function\n",
    "4. Decode and print the result\n",
    "5. Set model back to training mode\n",
    "```\n",
    "\n",
    "**Why generate during training?**\n",
    "- Loss numbers alone don't tell you if the model is generating coherent text\n",
    "- Seeing generated text helps you catch problems early\n",
    "- It's satisfying to watch the model improve!\n",
    "\n",
    "### 2. `evaluate_model`\n",
    "\n",
    "This function calculates loss on both training and validation sets:\n",
    "\n",
    "```python\n",
    "1. Set model to eval mode\n",
    "2. Calculate average loss on training data\n",
    "3. Calculate average loss on validation data\n",
    "4. Set model back to training mode\n",
    "5. Return both losses\n",
    "```\n",
    "\n",
    "We use `torch.no_grad()` during evaluation to:\n",
    "- Save memory (no gradients stored)\n",
    "- Speed up computation (no backward pass needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1f88ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b202dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_70",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "This is where the magic happens! `train_model_simple` implements the core **training loop** that teaches the model to predict next tokens.\n",
    "\n",
    "### The Training Process (Each Epoch):\n",
    "\n",
    "```\n",
    "For each epoch:\n",
    "    For each batch in training data:\n",
    "        1. Zero gradients           \u2190 Reset from previous step\n",
    "        2. Forward pass             \u2190 Compute predictions\n",
    "        3. Calculate loss           \u2190 Compare to targets\n",
    "        4. Backward pass            \u2190 Compute gradients\n",
    "        5. Update weights           \u2190 Apply gradients via optimizer\n",
    "        \n",
    "    Every eval_freq steps:\n",
    "        - Evaluate on train & val sets\n",
    "        - Log the losses\n",
    "        \n",
    "    At end of epoch:\n",
    "        - Generate sample text\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Step | What Happens | Why |\n",
    "|------|--------------|-----|\n",
    "| `optimizer.zero_grad()` | Reset gradients to zero | Gradients accumulate by default |\n",
    "| `loss.backward()` | Compute gradients via backprop | Determines how to update weights |\n",
    "| `optimizer.step()` | Update weights using gradients | The actual learning! |\n",
    "\n",
    "### Tracking Progress:\n",
    "\n",
    "The function returns three lists:\n",
    "- `train_losses`: Training loss over time\n",
    "- `val_losses`: Validation loss over time  \n",
    "- `tokens_seen`: Cumulative tokens processed\n",
    "\n",
    "These help us visualize learning curves and detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e3206f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_71",
   "metadata": {},
   "source": [
    "## Start Training!\n",
    "\n",
    "Now we instantiate everything and begin training:\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **Model**: Fresh GPTModel with random weights\n",
    "2. **Optimizer**: AdamW - the go-to optimizer for transformers\n",
    "3. **Hyperparameters**:\n",
    "   - `lr=0.0004`: Learning rate (how big each update step is)\n",
    "   - `weight_decay=0.1`: L2 regularization to prevent overfitting\n",
    "   - `num_epochs=10`: How many times to see the full dataset\n",
    "\n",
    "### AdamW vs Adam:\n",
    "\n",
    "AdamW **decouples weight decay** from the gradient update:\n",
    "- Regular Adam: applies weight decay to gradients (incorrect L2 regularization)\n",
    "- AdamW: applies weight decay directly to weights (correct L2 regularization)\n",
    "\n",
    "This is especially important for transformers!\n",
    "\n",
    "### What to Watch:\n",
    "\n",
    "During training, you'll see:\n",
    "```\n",
    "Ep 1 (Step 000005): Train loss 10.234, Val loss 10.456\n",
    "...\n",
    "Ep 10 (Step 000415): Train loss 0.678, Val loss 1.234\n",
    "```\n",
    "\n",
    "And sample generations that improve from gibberish to (somewhat) coherent text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22253752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.768, Val loss 9.840\n",
      "Ep 1 (Step 000005): Train loss 8.269, Val loss 8.392\n",
      "Ep 1 (Step 000010): Train loss 7.305, Val loss 7.642\n",
      "Ep 1 (Step 000015): Train loss 7.135, Val loss 7.364\n",
      "Ep 1 (Step 000020): Train loss 7.101, Val loss 7.295\n",
      "Ep 1 (Step 000025): Train loss 7.094, Val loss 7.192\n",
      "Ep 1 (Step 000030): Train loss 7.057, Val loss 7.095\n",
      "Ep 1 (Step 000035): Train loss 6.852, Val loss 7.002\n",
      "Ep 1 (Step 000040): Train loss 6.882, Val loss 6.933\n",
      "Ep 1 (Step 000045): Train loss 6.731, Val loss 6.876\n",
      "Ep 1 (Step 000050): Train loss 6.725, Val loss 6.805\n",
      "Ep 1 (Step 000055): Train loss 6.396, Val loss 6.726\n",
      "Ep 1 (Step 000060): Train loss 6.423, Val loss 6.695\n",
      "Ep 1 (Step 000065): Train loss 6.452, Val loss 6.631\n",
      "Ep 1 (Step 000070): Train loss 6.230, Val loss 6.604\n",
      "Ep 1 (Step 000075): Train loss 6.344, Val loss 6.568\n",
      "Ep 1 (Step 000080): Train loss 6.386, Val loss 6.488\n",
      "Ep 1 (Step 000085): Train loss 6.237, Val loss 6.443\n",
      "Ep 1 (Step 000090): Train loss 6.163, Val loss 6.428\n",
      "Ep 1 (Step 000095): Train loss 6.127, Val loss 6.394\n",
      "Ep 1 (Step 000100): Train loss 6.255, Val loss 6.387\n",
      "Ep 1 (Step 000105): Train loss 6.120, Val loss 6.382\n",
      "Ep 1 (Step 000110): Train loss 5.988, Val loss 6.361\n",
      "Ep 1 (Step 000115): Train loss 6.104, Val loss 6.353\n",
      "Ep 1 (Step 000120): Train loss 6.000, Val loss 6.341\n",
      "Ep 1 (Step 000125): Train loss 5.797, Val loss 6.303\n",
      "Ep 1 (Step 000130): Train loss 5.934, Val loss 6.275\n",
      "Ep 1 (Step 000135): Train loss 5.847, Val loss 6.247\n",
      "Ep 1 (Step 000140): Train loss 5.749, Val loss 6.192\n",
      "Ep 1 (Step 000145): Train loss 5.909, Val loss 6.199\n",
      "Ep 1 (Step 000150): Train loss 5.996, Val loss 6.166\n",
      "Ep 1 (Step 000155): Train loss 5.936, Val loss 6.154\n",
      "Ep 1 (Step 000160): Train loss 5.703, Val loss 6.161\n",
      "Ep 1 (Step 000165): Train loss 5.961, Val loss 6.147\n",
      "Ep 1 (Step 000170): Train loss 5.808, Val loss 6.152\n",
      "Ep 1 (Step 000175): Train loss 5.773, Val loss 6.127\n",
      "Ep 1 (Step 000180): Train loss 5.706, Val loss 6.097\n",
      "Ep 1 (Step 000185): Train loss 5.923, Val loss 6.094\n",
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000190): Train loss 5.691, Val loss 6.101\n",
      "Ep 2 (Step 000195): Train loss 5.653, Val loss 6.109\n",
      "Ep 2 (Step 000200): Train loss 5.636, Val loss 6.134\n",
      "Ep 2 (Step 000205): Train loss 5.887, Val loss 6.089\n",
      "Ep 2 (Step 000210): Train loss 5.858, Val loss 6.033\n",
      "Ep 2 (Step 000215): Train loss 5.731, Val loss 6.035\n",
      "Ep 2 (Step 000220): Train loss 5.695, Val loss 6.032\n",
      "Ep 2 (Step 000225): Train loss 5.668, Val loss 6.034\n",
      "Ep 2 (Step 000230): Train loss 5.604, Val loss 6.048\n",
      "Ep 2 (Step 000235): Train loss 5.472, Val loss 6.063\n",
      "Ep 2 (Step 000240): Train loss 5.680, Val loss 6.059\n",
      "Ep 2 (Step 000245): Train loss 5.639, Val loss 6.081\n",
      "Ep 2 (Step 000250): Train loss 5.546, Val loss 6.085\n",
      "Ep 2 (Step 000255): Train loss 5.586, Val loss 6.101\n",
      "Ep 2 (Step 000260): Train loss 5.647, Val loss 6.062\n",
      "Ep 2 (Step 000265): Train loss 5.522, Val loss 6.049\n",
      "Ep 2 (Step 000270): Train loss 5.573, Val loss 6.046\n",
      "Ep 2 (Step 000275): Train loss 5.538, Val loss 6.039\n",
      "Ep 2 (Step 000280): Train loss 5.512, Val loss 6.024\n",
      "Ep 2 (Step 000285): Train loss 5.553, Val loss 6.046\n",
      "Ep 2 (Step 000290): Train loss 5.628, Val loss 6.040\n",
      "Ep 2 (Step 000295): Train loss 5.513, Val loss 6.038\n",
      "Ep 2 (Step 000300): Train loss 5.564, Val loss 6.016\n",
      "Ep 2 (Step 000305): Train loss 5.395, Val loss 6.010\n",
      "Ep 2 (Step 000310): Train loss 5.448, Val loss 6.014\n",
      "Ep 2 (Step 000315): Train loss 5.508, Val loss 5.986\n",
      "Ep 2 (Step 000320): Train loss 5.338, Val loss 5.984\n",
      "Ep 2 (Step 000325): Train loss 5.416, Val loss 5.982\n",
      "Ep 2 (Step 000330): Train loss 5.652, Val loss 5.980\n",
      "Ep 2 (Step 000335): Train loss 5.324, Val loss 5.963\n",
      "Ep 2 (Step 000340): Train loss 5.388, Val loss 5.972\n",
      "Ep 2 (Step 000345): Train loss 5.433, Val loss 5.950\n",
      "Ep 2 (Step 000350): Train loss 5.417, Val loss 5.986\n",
      "Ep 2 (Step 000355): Train loss 5.253, Val loss 5.957\n",
      "Ep 2 (Step 000360): Train loss 5.624, Val loss 5.957\n",
      "Ep 2 (Step 000365): Train loss 5.273, Val loss 5.913\n",
      "Ep 2 (Step 000370): Train loss 5.513, Val loss 5.910\n",
      "Every effort moves you         \u201cI was \u201d \u201cI was \u201cI was  \u201d     \u201d \u201d    \u201d   \ufffd\n",
      "Ep 3 (Step 000375): Train loss 5.282, Val loss 5.913\n",
      "Ep 3 (Step 000380): Train loss 5.353, Val loss 5.925\n",
      "Ep 3 (Step 000385): Train loss 5.097, Val loss 5.913\n",
      "Ep 3 (Step 000390): Train loss 5.282, Val loss 5.920\n",
      "Ep 3 (Step 000395): Train loss 5.331, Val loss 5.934\n",
      "Ep 3 (Step 000400): Train loss 5.168, Val loss 5.927\n",
      "Ep 3 (Step 000405): Train loss 5.120, Val loss 5.953\n",
      "Ep 3 (Step 000410): Train loss 5.021, Val loss 5.946\n",
      "Ep 3 (Step 000415): Train loss 5.453, Val loss 5.989\n",
      "Ep 3 (Step 000420): Train loss 5.278, Val loss 5.970\n",
      "Ep 3 (Step 000425): Train loss 5.414, Val loss 5.940\n",
      "Ep 3 (Step 000430): Train loss 5.306, Val loss 5.942\n",
      "Ep 3 (Step 000435): Train loss 5.197, Val loss 5.938\n",
      "Ep 3 (Step 000440): Train loss 5.311, Val loss 5.921\n",
      "Ep 3 (Step 000445): Train loss 5.200, Val loss 5.967\n",
      "Ep 3 (Step 000450): Train loss 5.201, Val loss 5.951\n",
      "Ep 3 (Step 000455): Train loss 5.141, Val loss 5.905\n",
      "Ep 3 (Step 000460): Train loss 5.162, Val loss 5.908\n",
      "Ep 3 (Step 000465): Train loss 5.153, Val loss 5.893\n",
      "Ep 3 (Step 000470): Train loss 5.224, Val loss 5.908\n",
      "Ep 3 (Step 000475): Train loss 5.278, Val loss 5.912\n",
      "Ep 3 (Step 000480): Train loss 5.322, Val loss 5.918\n",
      "Ep 3 (Step 000485): Train loss 5.185, Val loss 5.884\n",
      "Ep 3 (Step 000490): Train loss 5.233, Val loss 5.876\n",
      "Ep 3 (Step 000495): Train loss 5.121, Val loss 5.869\n",
      "Ep 3 (Step 000500): Train loss 5.176, Val loss 5.904\n",
      "Ep 3 (Step 000505): Train loss 5.092, Val loss 5.879\n",
      "Ep 3 (Step 000510): Train loss 5.017, Val loss 5.909\n",
      "Ep 3 (Step 000515): Train loss 4.966, Val loss 5.892\n",
      "Ep 3 (Step 000520): Train loss 5.138, Val loss 5.917\n",
      "Ep 3 (Step 000525): Train loss 5.265, Val loss 5.878\n",
      "Ep 3 (Step 000530): Train loss 5.191, Val loss 5.868\n",
      "Ep 3 (Step 000535): Train loss 5.245, Val loss 5.853\n",
      "Ep 3 (Step 000540): Train loss 5.001, Val loss 5.875\n",
      "Ep 3 (Step 000545): Train loss 5.051, Val loss 5.859\n",
      "Ep 3 (Step 000550): Train loss 5.050, Val loss 5.819\n",
      "Ep 3 (Step 000555): Train loss 5.081, Val loss 5.840\n",
      "Ep 3 (Step 000560): Train loss 5.106, Val loss 5.794\n",
      "Every effort moves you, and I I have been a I have been I have been a I have been a I have been and I was a I have been a I had been I have been a I had not to\n",
      "Ep 4 (Step 000565): Train loss 4.935, Val loss 5.821\n",
      "Ep 4 (Step 000570): Train loss 5.052, Val loss 5.852\n",
      "Ep 4 (Step 000575): Train loss 4.797, Val loss 5.884\n",
      "Ep 4 (Step 000580): Train loss 4.938, Val loss 5.889\n",
      "Ep 4 (Step 000585): Train loss 5.171, Val loss 5.926\n",
      "Ep 4 (Step 000590): Train loss 4.920, Val loss 5.887\n",
      "Ep 4 (Step 000595): Train loss 5.045, Val loss 5.892\n",
      "Ep 4 (Step 000600): Train loss 4.948, Val loss 5.889\n",
      "Ep 4 (Step 000605): Train loss 4.998, Val loss 5.847\n",
      "Ep 4 (Step 000610): Train loss 4.955, Val loss 5.872\n",
      "Ep 4 (Step 000615): Train loss 4.804, Val loss 5.884\n",
      "Ep 4 (Step 000620): Train loss 4.929, Val loss 5.879\n",
      "Ep 4 (Step 000625): Train loss 4.786, Val loss 5.918\n",
      "Ep 4 (Step 000630): Train loss 4.897, Val loss 5.913\n",
      "Ep 4 (Step 000635): Train loss 5.135, Val loss 5.915\n",
      "Ep 4 (Step 000640): Train loss 5.087, Val loss 5.869\n",
      "Ep 4 (Step 000645): Train loss 4.815, Val loss 5.857\n",
      "Ep 4 (Step 000650): Train loss 4.939, Val loss 5.893\n",
      "Ep 4 (Step 000655): Train loss 4.705, Val loss 5.896\n",
      "Ep 4 (Step 000660): Train loss 4.886, Val loss 5.909\n",
      "Ep 4 (Step 000665): Train loss 4.976, Val loss 5.877\n",
      "Ep 4 (Step 000670): Train loss 4.957, Val loss 5.856\n",
      "Ep 4 (Step 000675): Train loss 4.760, Val loss 5.880\n",
      "Ep 4 (Step 000680): Train loss 4.730, Val loss 5.870\n",
      "Ep 4 (Step 000685): Train loss 4.789, Val loss 5.876\n",
      "Ep 4 (Step 000690): Train loss 4.837, Val loss 5.857\n",
      "Ep 4 (Step 000695): Train loss 4.664, Val loss 5.865\n",
      "Ep 4 (Step 000700): Train loss 4.767, Val loss 5.852\n",
      "Ep 4 (Step 000705): Train loss 4.713, Val loss 5.854\n",
      "Ep 4 (Step 000710): Train loss 4.883, Val loss 5.865\n",
      "Ep 4 (Step 000715): Train loss 4.798, Val loss 5.866\n",
      "Ep 4 (Step 000720): Train loss 4.687, Val loss 5.837\n",
      "Ep 4 (Step 000725): Train loss 4.691, Val loss 5.842\n",
      "Ep 4 (Step 000730): Train loss 4.664, Val loss 5.836\n",
      "Ep 4 (Step 000735): Train loss 4.567, Val loss 5.826\n",
      "Ep 4 (Step 000740): Train loss 4.858, Val loss 5.819\n",
      "Ep 4 (Step 000745): Train loss 4.702, Val loss 5.832\n",
      "Every effort moves you thewin, and I   the the the the the old man, and I the old man, and I the the old man, and I the the old man, I  the\n",
      "Ep 5 (Step 000750): Train loss 4.663, Val loss 5.834\n",
      "Ep 5 (Step 000755): Train loss 4.603, Val loss 5.873\n",
      "Ep 5 (Step 000760): Train loss 4.607, Val loss 5.892\n",
      "Ep 5 (Step 000765): Train loss 4.494, Val loss 5.887\n",
      "Ep 5 (Step 000770): Train loss 4.673, Val loss 5.887\n",
      "Ep 5 (Step 000775): Train loss 4.450, Val loss 5.909\n",
      "Ep 5 (Step 000780): Train loss 4.594, Val loss 5.902\n",
      "Ep 5 (Step 000785): Train loss 4.603, Val loss 5.901\n",
      "Ep 5 (Step 000790): Train loss 4.652, Val loss 5.921\n",
      "Ep 5 (Step 000795): Train loss 4.561, Val loss 5.898\n",
      "Ep 5 (Step 000800): Train loss 4.233, Val loss 5.920\n",
      "Ep 5 (Step 000805): Train loss 4.534, Val loss 5.934\n",
      "Ep 5 (Step 000810): Train loss 4.442, Val loss 5.905\n",
      "Ep 5 (Step 000815): Train loss 4.581, Val loss 5.909\n",
      "Ep 5 (Step 000820): Train loss 4.269, Val loss 5.988\n",
      "Ep 5 (Step 000825): Train loss 4.494, Val loss 5.920\n",
      "Ep 5 (Step 000830): Train loss 4.494, Val loss 5.901\n",
      "Ep 5 (Step 000835): Train loss 4.646, Val loss 5.906\n",
      "Ep 5 (Step 000840): Train loss 4.495, Val loss 5.908\n",
      "Ep 5 (Step 000845): Train loss 4.445, Val loss 5.965\n",
      "Ep 5 (Step 000850): Train loss 4.532, Val loss 5.933\n",
      "Ep 5 (Step 000855): Train loss 4.477, Val loss 5.920\n",
      "Ep 5 (Step 000860): Train loss 4.370, Val loss 5.892\n",
      "Ep 5 (Step 000865): Train loss 4.298, Val loss 5.887\n",
      "Ep 5 (Step 000870): Train loss 4.329, Val loss 5.929\n",
      "Ep 5 (Step 000875): Train loss 4.316, Val loss 5.938\n",
      "Ep 5 (Step 000880): Train loss 4.332, Val loss 5.930\n",
      "Ep 5 (Step 000885): Train loss 4.242, Val loss 5.883\n",
      "Ep 5 (Step 000890): Train loss 4.222, Val loss 5.899\n",
      "Ep 5 (Step 000895): Train loss 4.241, Val loss 5.914\n",
      "Ep 5 (Step 000900): Train loss 4.287, Val loss 5.917\n",
      "Ep 5 (Step 000905): Train loss 4.362, Val loss 5.904\n",
      "Ep 5 (Step 000910): Train loss 4.315, Val loss 5.878\n",
      "Ep 5 (Step 000915): Train loss 4.319, Val loss 5.892\n",
      "Ep 5 (Step 000920): Train loss 4.277, Val loss 5.848\n",
      "Ep 5 (Step 000925): Train loss 4.280, Val loss 5.849\n",
      "Ep 5 (Step 000930): Train loss 4.089, Val loss 5.885\n",
      "Every effort moves you.\u201d   \u201cI am not  \u201d    \u201d \u201d \u201d   \u201d \u201d     \u201d \u201d said I am not \n",
      "Ep 6 (Step 000935): Train loss 4.270, Val loss 5.874\n",
      "Ep 6 (Step 000940): Train loss 4.291, Val loss 5.935\n",
      "Ep 6 (Step 000945): Train loss 4.208, Val loss 5.898\n",
      "Ep 6 (Step 000950): Train loss 4.217, Val loss 5.920\n",
      "Ep 6 (Step 000955): Train loss 4.081, Val loss 5.924\n",
      "Ep 6 (Step 000960): Train loss 4.092, Val loss 5.929\n",
      "Ep 6 (Step 000965): Train loss 3.891, Val loss 5.937\n",
      "Ep 6 (Step 000970): Train loss 4.083, Val loss 5.960\n",
      "Ep 6 (Step 000975): Train loss 4.008, Val loss 6.023\n",
      "Ep 6 (Step 000980): Train loss 4.155, Val loss 5.981\n",
      "Ep 6 (Step 000985): Train loss 4.212, Val loss 6.007\n",
      "Ep 6 (Step 000990): Train loss 3.920, Val loss 5.998\n",
      "Ep 6 (Step 000995): Train loss 3.911, Val loss 6.034\n",
      "Ep 6 (Step 001000): Train loss 4.015, Val loss 6.035\n",
      "Ep 6 (Step 001005): Train loss 4.006, Val loss 5.995\n",
      "Ep 6 (Step 001010): Train loss 4.066, Val loss 6.022\n",
      "Ep 6 (Step 001015): Train loss 4.335, Val loss 6.049\n",
      "Ep 6 (Step 001020): Train loss 3.935, Val loss 6.030\n",
      "Ep 6 (Step 001025): Train loss 4.174, Val loss 6.040\n",
      "Ep 6 (Step 001030): Train loss 3.895, Val loss 6.049\n",
      "Ep 6 (Step 001035): Train loss 4.029, Val loss 6.000\n",
      "Ep 6 (Step 001040): Train loss 4.056, Val loss 6.046\n",
      "Ep 6 (Step 001045): Train loss 4.097, Val loss 6.008\n",
      "Ep 6 (Step 001050): Train loss 4.106, Val loss 6.007\n",
      "Ep 6 (Step 001055): Train loss 4.008, Val loss 6.084\n",
      "Ep 6 (Step 001060): Train loss 3.939, Val loss 6.060\n",
      "Ep 6 (Step 001065): Train loss 3.955, Val loss 6.021\n",
      "Ep 6 (Step 001070): Train loss 3.945, Val loss 5.989\n",
      "Ep 6 (Step 001075): Train loss 3.750, Val loss 6.025\n",
      "Ep 6 (Step 001080): Train loss 3.823, Val loss 6.041\n",
      "Ep 6 (Step 001085): Train loss 3.977, Val loss 6.005\n",
      "Ep 6 (Step 001090): Train loss 3.854, Val loss 5.992\n",
      "Ep 6 (Step 001095): Train loss 3.849, Val loss 5.993\n",
      "Ep 6 (Step 001100): Train loss 3.809, Val loss 5.978\n",
      "Ep 6 (Step 001105): Train loss 3.706, Val loss 5.988\n",
      "Ep 6 (Step 001110): Train loss 3.715, Val loss 5.994\n",
      "Ep 6 (Step 001115): Train loss 3.669, Val loss 6.014\n",
      "Ep 6 (Step 001120): Train loss 3.711, Val loss 6.009\n",
      "Every effort moves you, I have been and if I have been a and a and I have no longer, and and if the and, as a and a and a and a and me, and I have been so much\n",
      "Ep 7 (Step 001125): Train loss 3.728, Val loss 6.016\n",
      "Ep 7 (Step 001130): Train loss 3.805, Val loss 6.023\n",
      "Ep 7 (Step 001135): Train loss 3.592, Val loss 6.051\n",
      "Ep 7 (Step 001140): Train loss 3.662, Val loss 6.097\n",
      "Ep 7 (Step 001145): Train loss 3.617, Val loss 6.097\n",
      "Ep 7 (Step 001150): Train loss 3.536, Val loss 6.108\n",
      "Ep 7 (Step 001155): Train loss 3.598, Val loss 6.107\n",
      "Ep 7 (Step 001160): Train loss 3.610, Val loss 6.137\n",
      "Ep 7 (Step 001165): Train loss 3.481, Val loss 6.159\n",
      "Ep 7 (Step 001170): Train loss 3.524, Val loss 6.157\n",
      "Ep 7 (Step 001175): Train loss 3.482, Val loss 6.190\n",
      "Ep 7 (Step 001180): Train loss 3.624, Val loss 6.198\n",
      "Ep 7 (Step 001185): Train loss 3.762, Val loss 6.182\n",
      "Ep 7 (Step 001190): Train loss 3.425, Val loss 6.259\n",
      "Ep 7 (Step 001195): Train loss 3.843, Val loss 6.175\n",
      "Ep 7 (Step 001200): Train loss 3.653, Val loss 6.222\n",
      "Ep 7 (Step 001205): Train loss 3.094, Val loss 6.205\n",
      "Ep 7 (Step 001210): Train loss 3.574, Val loss 6.179\n",
      "Ep 7 (Step 001215): Train loss 3.177, Val loss 6.193\n",
      "Ep 7 (Step 001220): Train loss 3.595, Val loss 6.187\n",
      "Ep 7 (Step 001225): Train loss 3.312, Val loss 6.146\n",
      "Ep 7 (Step 001230): Train loss 3.633, Val loss 6.203\n",
      "Ep 7 (Step 001235): Train loss 3.235, Val loss 6.199\n",
      "Ep 7 (Step 001240): Train loss 3.415, Val loss 6.241\n",
      "Ep 7 (Step 001245): Train loss 3.421, Val loss 6.236\n",
      "Ep 7 (Step 001250): Train loss 3.361, Val loss 6.205\n",
      "Ep 7 (Step 001255): Train loss 3.344, Val loss 6.235\n",
      "Ep 7 (Step 001260): Train loss 3.171, Val loss 6.201\n",
      "Ep 7 (Step 001265): Train loss 3.328, Val loss 6.175\n",
      "Ep 7 (Step 001270): Train loss 3.264, Val loss 6.180\n",
      "Ep 7 (Step 001275): Train loss 3.287, Val loss 6.185\n",
      "Ep 7 (Step 001280): Train loss 3.272, Val loss 6.164\n",
      "Ep 7 (Step 001285): Train loss 3.292, Val loss 6.184\n",
      "Ep 7 (Step 001290): Train loss 3.207, Val loss 6.159\n",
      "Ep 7 (Step 001295): Train loss 3.345, Val loss 6.160\n",
      "Ep 7 (Step 001300): Train loss 3.272, Val loss 6.181\n",
      "Ep 7 (Step 001305): Train loss 3.253, Val loss 6.153\n",
      "Every effort moves you, but a the structure of your own handwritingure.   \u201c\u201c\u201c\u201c\u2018Do you will be the world.\u201c\u201c\u201c\u201c\u201c\u2018Do not be more \ufffd\n",
      "Ep 8 (Step 001310): Train loss 3.259, Val loss 6.139\n",
      "Ep 8 (Step 001315): Train loss 3.097, Val loss 6.194\n",
      "Ep 8 (Step 001320): Train loss 2.950, Val loss 6.214\n",
      "Ep 8 (Step 001325): Train loss 3.179, Val loss 6.266\n",
      "Ep 8 (Step 001330): Train loss 2.968, Val loss 6.300\n",
      "Ep 8 (Step 001335): Train loss 2.883, Val loss 6.324\n",
      "Ep 8 (Step 001340): Train loss 3.088, Val loss 6.345\n",
      "Ep 8 (Step 001345): Train loss 2.970, Val loss 6.353\n",
      "Ep 8 (Step 001350): Train loss 2.895, Val loss 6.356\n",
      "Ep 8 (Step 001355): Train loss 2.847, Val loss 6.341\n",
      "Ep 8 (Step 001360): Train loss 3.038, Val loss 6.331\n",
      "Ep 8 (Step 001365): Train loss 3.170, Val loss 6.326\n",
      "Ep 8 (Step 001370): Train loss 3.166, Val loss 6.404\n",
      "Ep 8 (Step 001375): Train loss 2.824, Val loss 6.379\n",
      "Ep 8 (Step 001380): Train loss 2.595, Val loss 6.377\n",
      "Ep 8 (Step 001385): Train loss 3.188, Val loss 6.398\n",
      "Ep 8 (Step 001390): Train loss 2.945, Val loss 6.351\n",
      "Ep 8 (Step 001395): Train loss 2.955, Val loss 6.336\n",
      "Ep 8 (Step 001400): Train loss 2.643, Val loss 6.407\n",
      "Ep 8 (Step 001405): Train loss 2.749, Val loss 6.452\n",
      "Ep 8 (Step 001410): Train loss 2.971, Val loss 6.351\n",
      "Ep 8 (Step 001415): Train loss 2.896, Val loss 6.385\n",
      "Ep 8 (Step 001420): Train loss 2.726, Val loss 6.384\n",
      "Ep 8 (Step 001425): Train loss 2.610, Val loss 6.407\n",
      "Ep 8 (Step 001430): Train loss 2.922, Val loss 6.420\n",
      "Ep 8 (Step 001435): Train loss 2.561, Val loss 6.434\n",
      "Ep 8 (Step 001440): Train loss 2.739, Val loss 6.434\n",
      "Ep 8 (Step 001445): Train loss 2.934, Val loss 6.422\n",
      "Ep 8 (Step 001450): Train loss 2.636, Val loss 6.416\n",
      "Ep 8 (Step 001455): Train loss 2.590, Val loss 6.395\n",
      "Ep 8 (Step 001460): Train loss 2.601, Val loss 6.391\n",
      "Ep 8 (Step 001465): Train loss 2.639, Val loss 6.395\n",
      "Ep 8 (Step 001470): Train loss 2.612, Val loss 6.398\n",
      "Ep 8 (Step 001475): Train loss 2.794, Val loss 6.353\n",
      "Ep 8 (Step 001480): Train loss 2.720, Val loss 6.394\n",
      "Ep 8 (Step 001485): Train loss 2.803, Val loss 6.416\n",
      "Ep 8 (Step 001490): Train loss 2.784, Val loss 6.410\n",
      "Ep 8 (Step 001495): Train loss 2.598, Val loss 6.350\n",
      "Every effort moves you, and delicate complexions.\u201cI will be the \u201d  \u201cI will repay me; and you to you \u201cI will be the you, \u201cI will be a man is your power\n",
      "Ep 9 (Step 001500): Train loss 2.479, Val loss 6.379\n",
      "Ep 9 (Step 001505): Train loss 2.555, Val loss 6.427\n",
      "Ep 9 (Step 001510): Train loss 2.259, Val loss 6.478\n",
      "Ep 9 (Step 001515): Train loss 2.565, Val loss 6.494\n",
      "Ep 9 (Step 001520): Train loss 2.465, Val loss 6.505\n",
      "Ep 9 (Step 001525): Train loss 2.390, Val loss 6.517\n",
      "Ep 9 (Step 001530): Train loss 2.391, Val loss 6.566\n",
      "Ep 9 (Step 001535): Train loss 2.474, Val loss 6.562\n",
      "Ep 9 (Step 001540): Train loss 2.433, Val loss 6.559\n",
      "Ep 9 (Step 001545): Train loss 1.969, Val loss 6.588\n",
      "Ep 9 (Step 001550): Train loss 2.449, Val loss 6.564\n",
      "Ep 9 (Step 001555): Train loss 2.039, Val loss 6.550\n",
      "Ep 9 (Step 001560): Train loss 2.197, Val loss 6.577\n",
      "Ep 9 (Step 001565): Train loss 2.150, Val loss 6.610\n",
      "Ep 9 (Step 001570): Train loss 2.421, Val loss 6.623\n",
      "Ep 9 (Step 001575): Train loss 2.472, Val loss 6.603\n",
      "Ep 9 (Step 001580): Train loss 2.029, Val loss 6.602\n",
      "Ep 9 (Step 001585): Train loss 2.485, Val loss 6.615\n",
      "Ep 9 (Step 001590): Train loss 2.328, Val loss 6.642\n",
      "Ep 9 (Step 001595): Train loss 2.434, Val loss 6.694\n",
      "Ep 9 (Step 001600): Train loss 2.331, Val loss 6.726\n",
      "Ep 9 (Step 001605): Train loss 2.428, Val loss 6.666\n",
      "Ep 9 (Step 001610): Train loss 2.207, Val loss 6.599\n",
      "Ep 9 (Step 001615): Train loss 2.124, Val loss 6.634\n",
      "Ep 9 (Step 001620): Train loss 2.311, Val loss 6.634\n",
      "Ep 9 (Step 001625): Train loss 2.111, Val loss 6.670\n",
      "Ep 9 (Step 001630): Train loss 2.233, Val loss 6.687\n",
      "Ep 9 (Step 001635): Train loss 2.002, Val loss 6.638\n",
      "Ep 9 (Step 001640): Train loss 2.304, Val loss 6.670\n",
      "Ep 9 (Step 001645): Train loss 2.085, Val loss 6.644\n",
      "Ep 9 (Step 001650): Train loss 2.219, Val loss 6.653\n",
      "Ep 9 (Step 001655): Train loss 2.149, Val loss 6.661\n",
      "Ep 9 (Step 001660): Train loss 2.306, Val loss 6.650\n",
      "Ep 9 (Step 001665): Train loss 2.132, Val loss 6.662\n",
      "Ep 9 (Step 001670): Train loss 2.052, Val loss 6.662\n",
      "Ep 9 (Step 001675): Train loss 2.001, Val loss 6.646\n",
      "Ep 9 (Step 001680): Train loss 2.060, Val loss 6.716\n",
      "Every effort moves you; but the the court in this anderer, who has been the world, and the earth.\u201d        \u201cI am advancing, I am no comfort which I am taken refuge, but it\n",
      "Ep 10 (Step 001685): Train loss 2.019, Val loss 6.646\n",
      "Ep 10 (Step 001690): Train loss 1.805, Val loss 6.628\n",
      "Ep 10 (Step 001695): Train loss 2.012, Val loss 6.706\n",
      "Ep 10 (Step 001700): Train loss 1.936, Val loss 6.778\n",
      "Ep 10 (Step 001705): Train loss 1.840, Val loss 6.783\n",
      "Ep 10 (Step 001710): Train loss 1.955, Val loss 6.841\n",
      "Ep 10 (Step 001715): Train loss 1.938, Val loss 6.833\n",
      "Ep 10 (Step 001720): Train loss 1.952, Val loss 6.870\n",
      "Ep 10 (Step 001725): Train loss 1.929, Val loss 6.892\n",
      "Ep 10 (Step 001730): Train loss 1.868, Val loss 6.894\n",
      "Ep 10 (Step 001735): Train loss 1.780, Val loss 6.909\n",
      "Ep 10 (Step 001740): Train loss 1.981, Val loss 6.937\n",
      "Ep 10 (Step 001745): Train loss 1.929, Val loss 6.903\n",
      "Ep 10 (Step 001750): Train loss 1.958, Val loss 6.924\n",
      "Ep 10 (Step 001755): Train loss 1.798, Val loss 6.946\n",
      "Ep 10 (Step 001760): Train loss 1.855, Val loss 6.895\n",
      "Ep 10 (Step 001765): Train loss 1.948, Val loss 6.896\n",
      "Ep 10 (Step 001770): Train loss 1.890, Val loss 6.918\n",
      "Ep 10 (Step 001775): Train loss 1.696, Val loss 6.942\n",
      "Ep 10 (Step 001780): Train loss 1.817, Val loss 6.879\n",
      "Ep 10 (Step 001785): Train loss 1.609, Val loss 6.860\n",
      "Ep 10 (Step 001790): Train loss 1.722, Val loss 6.872\n",
      "Ep 10 (Step 001795): Train loss 1.860, Val loss 6.964\n",
      "Ep 10 (Step 001800): Train loss 1.807, Val loss 6.931\n",
      "Ep 10 (Step 001805): Train loss 1.670, Val loss 6.972\n",
      "Ep 10 (Step 001810): Train loss 1.818, Val loss 6.953\n",
      "Ep 10 (Step 001815): Train loss 1.491, Val loss 6.937\n",
      "Ep 10 (Step 001820): Train loss 1.453, Val loss 6.985\n",
      "Ep 10 (Step 001825): Train loss 1.618, Val loss 6.942\n",
      "Ep 10 (Step 001830): Train loss 1.630, Val loss 6.907\n",
      "Ep 10 (Step 001835): Train loss 1.495, Val loss 6.922\n",
      "Ep 10 (Step 001840): Train loss 1.632, Val loss 6.904\n",
      "Ep 10 (Step 001845): Train loss 1.609, Val loss 6.984\n",
      "Ep 10 (Step 001850): Train loss 1.463, Val loss 6.943\n",
      "Ep 10 (Step 001855): Train loss 1.605, Val loss 6.948\n",
      "Ep 10 (Step 001860): Train loss 1.492, Val loss 6.907\n",
      "Ep 10 (Step 001865): Train loss 1.588, Val loss 6.920\n",
      "Every effort moves you that a and from the edges of a more cheerfulies. The    and the earth had been discovered more cheerful hours and the same anderer, a half with the snow, a of these in a more than the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_72",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress\n",
    "\n",
    "Plotting the loss curves is essential for understanding how training went.\n",
    "\n",
    "### What the Plot Shows:\n",
    "\n",
    "- **X-axis (bottom)**: Epochs (complete passes through training data)\n",
    "- **X-axis (top)**: Tokens seen (cumulative count)\n",
    "- **Y-axis**: Cross-entropy loss\n",
    "- **Blue line**: Training loss\n",
    "- **Dashed line**: Validation loss\n",
    "\n",
    "### How to Read the Curves:\n",
    "\n",
    "| Pattern | Meaning | Action |\n",
    "|---------|---------|--------|\n",
    "| Both decreasing together | Healthy training | Continue! |\n",
    "| Train \u2193, Val flat | Starting to overfit | Consider stopping |\n",
    "| Train \u2193\u2193, Val \u2191 | Overfitting | Stop training, add regularization |\n",
    "| Both flat | Learning stalled | Increase learning rate |\n",
    "| Both erratic | LR too high | Decrease learning rate |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Gap between curves**: Some gap is normal; large gap = overfitting\n",
    "2. **Diminishing returns**: Improvement slows as loss decreases\n",
    "3. **Validation is truth**: Training loss can always go down; validation shows real generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d13e627a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_73",
   "metadata": {},
   "source": [
    "## Switching to CPU for Inference\n",
    "\n",
    "After training, we:\n",
    "1. Move the model to CPU (if it was on GPU)\n",
    "2. Set to evaluation mode with `model.eval()`\n",
    "\n",
    "### Why Move to CPU?\n",
    "\n",
    "- **Memory**: Frees up GPU memory for other tasks\n",
    "- **Portability**: CPU inference works everywhere\n",
    "- **Saving**: Easier to save/load models on CPU\n",
    "\n",
    "### Why `model.eval()`?\n",
    "\n",
    "Switches the model from training mode to evaluation mode:\n",
    "- **Dropout**: Disabled (uses all neurons)\n",
    "- **BatchNorm**: Uses running statistics (not applicable here, but good habit)\n",
    "- Ensures consistent, reproducible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eec7721b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_74",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's see what our trained model generates!\n",
    "\n",
    "### Before Training (Random Weights):\n",
    "```\n",
    "\"Every effort moves you rendezvous \n",
    "Ede diameterEde diameter Ede...\"\n",
    "```\n",
    "\n",
    "### After Training:\n",
    "The model should now generate more coherent, Frankenstein-like text. It won't be perfect (we only trained on one book for a short time), but it should:\n",
    "- Use real English words\n",
    "- Follow basic grammar\n",
    "- Sound somewhat like the training text\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "Our model is:\n",
    "- **Small**: 124M parameters (GPT-4 has ~1.8T)\n",
    "- **Undertrained**: Only saw ~1M tokens (GPT-3 saw 300B)\n",
    "- **Limited data**: Only one book (real LLMs train on internet-scale data)\n",
    "\n",
    "But it demonstrates the core concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6ef6254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you\n",
      "that a\n",
      "and from the edges of a more cheerfulies. The\n",
      "\n",
      "\n",
      "\n",
      "and the earth had been discovered\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Decoding Strategies - Temperature and Top-k Sampling\n",
    "\n",
    "So far we've used **greedy decoding**: always pick the highest probability token. This is deterministic but can be boring and repetitive.\n",
    "\n",
    "## Better Decoding Strategies\n",
    "\n",
    "Real language models use **sampling** with various controls:\n",
    "\n",
    "1. **Temperature**: Controls randomness/creativity\n",
    "2. **Top-k Sampling**: Only sample from the k most likely tokens\n",
    "3. **Top-p (Nucleus) Sampling**: Sample from tokens covering p% probability mass\n",
    "\n",
    "## Setting Up a Simple Example\n",
    "\n",
    "Let's use a tiny vocabulary to understand these concepts clearly:\n",
    "\n",
    "```\n",
    "vocab = {closer, every, effort, forward, inches, moves, pizza, toward, you}\n",
    "```\n",
    "\n",
    "We'll explore how different decoding strategies affect which token gets selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41102644",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_76",
   "metadata": {},
   "source": [
    "### Example Logits\n",
    "\n",
    "These are example **logits** (raw model outputs before softmax) for predicting the next token after \"Every effort moves you\":\n",
    "\n",
    "```\n",
    "forward: 6.75  \u2190 Highest logit\n",
    "toward:  6.28  \u2190 Second highest\n",
    "closer:  4.51\n",
    "you:     1.79\n",
    "inches:  1.63\n",
    "every:   0.89\n",
    "moves:  -1.62\n",
    "pizza:  -1.89\n",
    "effort: -1.90  \u2190 Lowest logit\n",
    "```\n",
    "\n",
    "**Note**: \"forward\" and \"toward\" make sense as next words (\"Every effort moves you forward/toward...\"), while \"pizza\" doesn't!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9005edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_77",
   "metadata": {},
   "source": [
    "### Greedy Decoding (Argmax)\n",
    "\n",
    "With greedy decoding, we:\n",
    "1. Convert logits to probabilities via softmax\n",
    "2. Pick the token with highest probability (argmax)\n",
    "\n",
    "This **always** picks \"forward\" because it has the highest logit/probability.\n",
    "\n",
    "### Pros and Cons:\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Deterministic | Repetitive output |\n",
    "| Fast | Misses creative alternatives |\n",
    "| Predictable | Can get stuck in loops |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5584808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_78",
   "metadata": {},
   "source": [
    "### Random Sampling (Multinomial)\n",
    "\n",
    "Instead of always picking the best, we **sample randomly** according to the probability distribution:\n",
    "\n",
    "- \"forward\" (p=0.45): Most likely to be picked, but not always\n",
    "- \"toward\" (p=0.30): Second most likely\n",
    "- \"closer\" (p=0.05): Occasionally picked\n",
    "- etc.\n",
    "\n",
    "`torch.multinomial(probas, num_samples=1)` samples one token according to these probabilities.\n",
    "\n",
    "### Why Sample?\n",
    "\n",
    "- **Diversity**: Different runs produce different outputs\n",
    "- **Creativity**: Explores less obvious continuations\n",
    "- **Natural**: Real language has variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1323ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_79",
   "metadata": {},
   "source": [
    "### Visualizing Sampling Distribution\n",
    "\n",
    "Let's sample 1000 tokens to see the actual distribution.\n",
    "\n",
    "With pure sampling, the frequency of each token should roughly match its probability:\n",
    "- \"forward\" appears ~450 times (45%)\n",
    "- \"toward\" appears ~300 times (30%)\n",
    "- Rare tokens appear occasionally\n",
    "\n",
    "This shows sampling respects the probability distribution while adding variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67f8dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "             for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_80",
   "metadata": {},
   "source": [
    "## Temperature Scaling\n",
    "\n",
    "**Temperature** controls how \"peaked\" or \"flat\" the probability distribution is.\n",
    "\n",
    "### The Formula:\n",
    "\n",
    "```python\n",
    "scaled_logits = logits / temperature\n",
    "probas = softmax(scaled_logits)\n",
    "```\n",
    "\n",
    "### Effect of Temperature:\n",
    "\n",
    "| Temperature | Effect | Use Case |\n",
    "|-------------|--------|----------|\n",
    "| T < 1.0 | Sharper distribution (more confident) | Factual, focused output |\n",
    "| T = 1.0 | Original distribution | Balanced |\n",
    "| T > 1.0 | Flatter distribution (more random) | Creative writing |\n",
    "| T \u2192 0 | Approaches greedy (argmax) | Most predictable |\n",
    "| T \u2192 \u221e | Approaches uniform random | Maximum chaos |\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "- **Low temperature** (T=0.1): \"Be very confident, stick to high-probability tokens\"\n",
    "- **High temperature** (T=5.0): \"Be adventurous, consider unlikely tokens too\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "377a8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_81",
   "metadata": {},
   "source": [
    "### Visualizing Temperature Effects\n",
    "\n",
    "This plot shows how temperature changes the probability distribution:\n",
    "\n",
    "- **T=1.0** (baseline): Original distribution from the model\n",
    "- **T=0.1** (cold): Almost all probability on \"forward\" (very peaked)\n",
    "- **T=5.0** (hot): Much more uniform distribution across all tokens\n",
    "\n",
    "### Practical Usage:\n",
    "\n",
    "```\n",
    "Chat/Q&A \u2192 T=0.3-0.7 (focused, accurate)\n",
    "Creative writing \u2192 T=0.8-1.2 (balanced creativity)\n",
    "Brainstorming \u2192 T=1.2-1.5 (more diverse ideas)\n",
    "```\n",
    "\n",
    "**Warning**: Temperature > 2.0 often produces incoherent text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8492ebf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_82",
   "metadata": {},
   "source": [
    "## Top-k Sampling\n",
    "\n",
    "**Top-k sampling** restricts sampling to only the k most probable tokens.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. Get logits for all vocabulary tokens\n",
    "2. Find the k highest logits\n",
    "3. Set all other logits to -infinity (zero probability after softmax)\n",
    "4. Sample from the remaining k tokens\n",
    "\n",
    "### Example with k=3:\n",
    "\n",
    "```\n",
    "Original:  [forward: 6.75, toward: 6.28, closer: 4.51, ...]\n",
    "After top-3: [forward: 6.75, toward: 6.28, closer: 4.51, \n",
    "              others: -inf, -inf, -inf, ...]\n",
    "```\n",
    "\n",
    "Now we can only sample from {forward, toward, closer}!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5978ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_83",
   "metadata": {},
   "source": [
    "### Implementing Top-k: Masking Low-Probability Tokens\n",
    "\n",
    "`torch.where` conditionally replaces values:\n",
    "- Tokens with logit < min(top-k logits) \u2192 set to -infinity\n",
    "- Tokens in top-k \u2192 keep original logit\n",
    "\n",
    "After this masking:\n",
    "```\n",
    "forward:  6.75  \u2190 kept\n",
    "toward:   6.28  \u2190 kept\n",
    "closer:   4.51  \u2190 kept (this is the k-th highest)\n",
    "you:     -inf   \u2190 masked out\n",
    "inches:  -inf   \u2190 masked out\n",
    "...\n",
    "```\n",
    "\n",
    "The -infinity values become 0 probability after softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a90d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_84",
   "metadata": {},
   "source": [
    "### Final Top-k Probabilities\n",
    "\n",
    "After applying softmax to the masked logits:\n",
    "\n",
    "```\n",
    "forward: ~50%\n",
    "toward:  ~35%\n",
    "closer:  ~15%\n",
    "others:   0%\n",
    "```\n",
    "\n",
    "Now sampling can only pick from these three sensible options!\n",
    "\n",
    "### Why Top-k?\n",
    "\n",
    "| Benefit | Explanation |\n",
    "|---------|-------------|\n",
    "| Prevents nonsense | Low-probability tokens (like \"pizza\") are excluded |\n",
    "| Maintains diversity | Still samples, doesn't always pick the best |\n",
    "| Controllable | k=1 is greedy, k=vocab_size is pure sampling |\n",
    "\n",
    "### Choosing k:\n",
    "\n",
    "- `k=1`: Greedy decoding\n",
    "- `k=10-50`: Common for most tasks\n",
    "- `k=100+`: Very diverse, might include odd tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a44f5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_85",
   "metadata": {},
   "source": [
    "## Advanced Generation Function\n",
    "\n",
    "Now we combine everything into a production-ready generation function with:\n",
    "\n",
    "1. **Temperature scaling**: Control creativity/randomness\n",
    "2. **Top-k filtering**: Exclude low-probability tokens\n",
    "3. **Early stopping**: Stop generation at end-of-sequence token\n",
    "\n",
    "### Function Parameters:\n",
    "\n",
    "| Parameter | Purpose |\n",
    "|-----------|---------|\n",
    "| `model` | The trained GPT model |\n",
    "| `idx` | Starting token IDs |\n",
    "| `max_new_tokens` | Maximum tokens to generate |\n",
    "| `context_size` | Maximum context window |\n",
    "| `temperature` | Creativity control (0.0 = greedy) |\n",
    "| `top_k` | Number of top tokens to sample from |\n",
    "| `eos_id` | End-of-sequence token to stop at |\n",
    "\n",
    "### Generation Logic:\n",
    "\n",
    "```\n",
    "For each new token:\n",
    "    1. Get logits from model\n",
    "    2. Apply top-k filtering (if enabled)\n",
    "    3. Apply temperature scaling (if temp > 0)\n",
    "    4. Sample or argmax depending on temperature\n",
    "    5. Check for EOS token\n",
    "    6. Append new token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09dba5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_86",
   "metadata": {},
   "source": [
    "### Testing the Advanced Generator\n",
    "\n",
    "Let's generate with:\n",
    "- `top_k=25`: Only consider top 25 tokens\n",
    "- `temperature=1.4`: Slightly creative/random\n",
    "\n",
    "Compare this to our earlier greedy generation:\n",
    "- **Greedy**: Always the same output, might be repetitive\n",
    "- **With sampling**: Different each run, more natural variety\n",
    "\n",
    "### Experimenting with Settings:\n",
    "\n",
    "Try different combinations:\n",
    "```python\n",
    "# Focused, deterministic\n",
    "generate(..., temperature=0.0, top_k=None)\n",
    "\n",
    "# Balanced\n",
    "generate(..., temperature=0.7, top_k=50)\n",
    "\n",
    "# Very creative\n",
    "generate(..., temperature=1.5, top_k=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3e348b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you not one an tale only a lively form? You\n",
      "I shall accompany\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Saving and Loading Models\n",
    "\n",
    "Training is expensive! We need to save our trained models to:\n",
    "- Resume training later\n",
    "- Deploy for inference\n",
    "- Share with others\n",
    "\n",
    "## Method 1: Save Just the Weights\n",
    "\n",
    "`torch.save(model.state_dict(), \"model.pth\")` saves:\n",
    "- All learned parameters (weights and biases)\n",
    "- Does NOT save: model architecture, optimizer state, training progress\n",
    "\n",
    "### File Format:\n",
    "\n",
    "- `.pth` or `.pt`: PyTorch convention (actually pickle format)\n",
    "- Contains a dictionary mapping parameter names to tensors\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "- Final model deployment\n",
    "- Sharing trained weights\n",
    "- When you don't need to resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "984e28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_88",
   "metadata": {},
   "source": [
    "### Loading Saved Weights\n",
    "\n",
    "To load saved weights:\n",
    "\n",
    "1. **Create the model architecture** (must match saved weights exactly!)\n",
    "2. **Load the state dict** from file\n",
    "3. **Set to eval mode** for inference\n",
    "\n",
    "```python\n",
    "model = GPTModel(CONFIG)           # 1. Same architecture\n",
    "model.load_state_dict(torch.load(\"model.pth\"))  # 2. Load weights\n",
    "model.eval()                        # 3. Evaluation mode\n",
    "```\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- Architecture must be identical (same config)\n",
    "- `map_location` helps load GPU-trained models on CPU\n",
    "- Always call `model.eval()` before inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e301c4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_89",
   "metadata": {},
   "source": [
    "## Method 2: Save Model + Optimizer (Checkpoint)\n",
    "\n",
    "For **resuming training**, we need to save more:\n",
    "- Model weights\n",
    "- Optimizer state (momentum, learning rate schedules, etc.)\n",
    "- Optionally: epoch number, loss history, random state\n",
    "\n",
    "### Why Save Optimizer State?\n",
    "\n",
    "Optimizers like AdamW maintain:\n",
    "- **Momentum terms**: Running averages of gradients\n",
    "- **Adaptive learning rates**: Per-parameter learning rate scaling\n",
    "\n",
    "Without these, resumed training starts \"cold\" and may perform worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a50f8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_90",
   "metadata": {},
   "source": [
    "### Loading a Full Checkpoint\n",
    "\n",
    "To resume training:\n",
    "\n",
    "1. Load the checkpoint dictionary\n",
    "2. Create fresh model and optimizer\n",
    "3. Load state dicts into both\n",
    "4. Set model to training mode\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()  # Ready to continue training!\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Save checkpoints periodically during training\n",
    "- Include epoch/step number in filename\n",
    "- Keep the best model based on validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63c3a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Loading Pre-trained GPT-2 Weights\n",
    "\n",
    "Training a model from scratch is educational, but for real applications we often start with **pre-trained weights**. Let's load the official GPT-2 weights from OpenAI!\n",
    "\n",
    "## Why Use Pre-trained Weights?\n",
    "\n",
    "| Training from Scratch | Using Pre-trained |\n",
    "|----------------------|-------------------|\n",
    "| Random initialization | Trained on WebText |\n",
    "| Need lots of data | Already knows language |\n",
    "| Expensive ($$, time) | Free and instant |\n",
    "| Custom architecture | Must match architecture |\n",
    "\n",
    "## Installing Dependencies\n",
    "\n",
    "We need TensorFlow to load GPT-2's original checkpoint format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d06aca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow>=2.15.0  tqdm>=4.66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_92",
   "metadata": {},
   "source": [
    "### Download Weight Loading Script\n",
    "\n",
    "We'll use a helper script that handles downloading GPT-2 weights from OpenAI's servers and converting them to a format we can use.\n",
    "\n",
    "The script:\n",
    "1. Downloads the GPT-2 checkpoint files\n",
    "2. Loads TensorFlow weights\n",
    "3. Converts to a Python dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e883f62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7fba45290590>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_93",
   "metadata": {},
   "source": [
    "### Download and Load GPT-2 Weights\n",
    "\n",
    "This downloads the **124M parameter** GPT-2 model (~500MB):\n",
    "\n",
    "```\n",
    "models/gpt2/\n",
    "\u251c\u2500\u2500 checkpoint          # TF checkpoint metadata\n",
    "\u251c\u2500\u2500 encoder.json        # Vocabulary\n",
    "\u251c\u2500\u2500 hparams.json        # Model config\n",
    "\u251c\u2500\u2500 model.ckpt.data     # The actual weights\n",
    "\u251c\u2500\u2500 model.ckpt.index    # Weight index\n",
    "\u2514\u2500\u2500 vocab.bpe           # BPE merges\n",
    "```\n",
    "\n",
    "The function returns:\n",
    "- `settings`: Model hyperparameters (dimensions, layers, etc.)\n",
    "- `params`: Dictionary of all weight tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "031d8c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77.0/77.0 [00:00<00:00, 198kiB/s]\n",
      "encoder.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.04M/1.04M [00:00<00:00, 1.18MiB/s]\n",
      "hparams.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 90.0/90.0 [00:00<00:00, 247kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 498M/498M [01:29<00:00, 5.56MiB/s] \n",
      "model.ckpt.index: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.21k/5.21k [00:00<00:00, 11.7MiB/s]\n",
      "model.ckpt.meta: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 471k/471k [00:00<00:00, 908kiB/s] \n",
      "vocab.bpe: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k [00:00<00:00, 616kiB/s] \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_94",
   "metadata": {},
   "source": [
    "### Examine GPT-2 Configuration\n",
    "\n",
    "Let's see what's in the loaded data:\n",
    "\n",
    "**Settings** contain architecture hyperparameters:\n",
    "```python\n",
    "{\n",
    "    \"n_vocab\": 50257,   # Vocabulary size\n",
    "    \"n_ctx\": 1024,      # Context length\n",
    "    \"n_embd\": 768,      # Embedding dimension\n",
    "    \"n_head\": 12,       # Attention heads\n",
    "    \"n_layer\": 12       # Transformer blocks\n",
    "}\n",
    "```\n",
    "\n",
    "**Params** contain the actual weight tensors:\n",
    "- `wte`: Token embeddings\n",
    "- `wpe`: Position embeddings  \n",
    "- `blocks`: List of transformer block weights\n",
    "- `g`, `b`: Final layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e66d4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_95",
   "metadata": {},
   "source": [
    "### Inspect Token Embeddings\n",
    "\n",
    "The `wte` (word token embeddings) matrix has shape `[50257, 768]`:\n",
    "- 50,257 tokens in the vocabulary\n",
    "- 768-dimensional embedding for each token\n",
    "\n",
    "These embeddings encode the **meaning** of each token, learned from billions of words of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58297635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_96",
   "metadata": {},
   "source": [
    "### GPT-2 Model Sizes\n",
    "\n",
    "GPT-2 comes in four sizes. Here are their configurations:\n",
    "\n",
    "| Model | Parameters | Layers | Dim | Heads |\n",
    "|-------|-----------|--------|-----|-------|\n",
    "| Small | 124M | 12 | 768 | 12 |\n",
    "| Medium | 355M | 24 | 1024 | 16 |\n",
    "| Large | 774M | 36 | 1280 | 20 |\n",
    "| XL | 1.5B | 48 | 1600 | 25 |\n",
    "\n",
    "**Scaling pattern**: Deeper (more layers) AND wider (larger dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d10ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_97",
   "metadata": {},
   "source": [
    "### Update Config for GPT-2\n",
    "\n",
    "We need to update our model configuration to exactly match GPT-2's architecture.\n",
    "\n",
    "Starting from our base config, we update:\n",
    "- `emb_dim`: Embedding dimension (768 for small)\n",
    "- `n_layers`: Number of transformer blocks (12 for small)\n",
    "- `n_heads`: Number of attention heads (12 for small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d477821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_98",
   "metadata": {},
   "source": [
    "### Set Correct Context Length\n",
    "\n",
    "GPT-2 was trained with a context length of **1024 tokens**, not 256 like our training runs.\n",
    "\n",
    "This must match exactly because:\n",
    "- Position embeddings have shape `[context_length, emb_dim]`\n",
    "- The causal mask is created for this size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd181aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_99",
   "metadata": {},
   "source": [
    "### Enable QKV Bias\n",
    "\n",
    "GPT-2 uses **biases in the Q, K, V projections**:\n",
    "```python\n",
    "qkv_bias = True\n",
    "```\n",
    "\n",
    "Our implementation had `qkv_bias=False` for training, but we need `True` to load GPT-2 weights.\n",
    "\n",
    "**Note**: This is a common difference between model variants. Always check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c810edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_100",
   "metadata": {},
   "source": [
    "### Create Compatible Model\n",
    "\n",
    "Now we create a fresh `GPTModel` with the updated configuration.\n",
    "\n",
    "This model has:\n",
    "- Correct architecture matching GPT-2\n",
    "- Random weights (we'll replace these next)\n",
    "- 1024 context length\n",
    "- QKV biases enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d1b9c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_101",
   "metadata": {},
   "source": [
    "### Weight Assignment Helper\n",
    "\n",
    "The `assign` function safely copies weights:\n",
    "\n",
    "1. **Shape check**: Ensures source and target shapes match exactly\n",
    "2. **Convert to Parameter**: Wraps the tensor as a learnable parameter\n",
    "3. **Error handling**: Fails loudly if shapes don't match\n",
    "\n",
    "This catches bugs early - mismatched shapes usually mean architecture differences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0901bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                          \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_102",
   "metadata": {},
   "source": [
    "## Loading All GPT-2 Weights\n",
    "\n",
    "`load_weights_into_gpt` is the main function that copies all weights from the GPT-2 checkpoint into our model.\n",
    "\n",
    "### Weight Mapping:\n",
    "\n",
    "| GPT-2 Name | Our Name | What It Is |\n",
    "|------------|----------|-----------|\n",
    "| `wpe` | `pos_emb.weight` | Position embeddings |\n",
    "| `wte` | `tok_emb.weight` | Token embeddings |\n",
    "| `blocks[i].attn.c_attn.w` | `W_query/key/value` | Attention projections |\n",
    "| `blocks[i].attn.c_proj` | `out_proj` | Attention output |\n",
    "| `blocks[i].mlp.c_fc` | `ff.layers[0]` | FFN first layer |\n",
    "| `blocks[i].mlp.c_proj` | `ff.layers[2]` | FFN second layer |\n",
    "| `blocks[i].ln_1` | `norm1` | First layer norm |\n",
    "| `blocks[i].ln_2` | `norm2` | Second layer norm |\n",
    "\n",
    "### Key Detail: Transposing Weights\n",
    "\n",
    "GPT-2 stores weights in `[in, out]` format, but PyTorch Linear layers expect `[out, in]`. We transpose with `.T` to fix this.\n",
    "\n",
    "### Weight Tying\n",
    "\n",
    "Notice `out_head.weight = wte` - GPT-2 **ties** output and input embeddings, meaning the same weights are used for both token lookup and prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb2998b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_103",
   "metadata": {},
   "source": [
    "### Apply Weights to Model\n",
    "\n",
    "Load all the weights and move the model to the appropriate device (GPU if available).\n",
    "\n",
    "After this, our model contains the exact same weights as OpenAI's GPT-2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5c619a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_insert_104",
   "metadata": {},
   "source": [
    "## Test Pre-trained GPT-2!\n",
    "\n",
    "Now we can generate text using the pre-trained GPT-2 weights.\n",
    "\n",
    "### The Difference:\n",
    "\n",
    "**Our trained model** (on Frankenstein):\n",
    "- Limited vocabulary of concepts\n",
    "- Frankenstein-style writing\n",
    "- ~1M tokens of training\n",
    "\n",
    "**Pre-trained GPT-2**:\n",
    "- Knows about everything on the internet (circa 2019)\n",
    "- Diverse writing styles\n",
    "- ~40GB of text (~8M web pages)\n",
    "\n",
    "### Try Different Prompts:\n",
    "\n",
    "```python\n",
    "\"The meaning of life is\"\n",
    "\"In a shocking turn of events,\"\n",
    "\"def fibonacci(n):\"\n",
    "```\n",
    "\n",
    "GPT-2 can complete all of these sensibly because it learned from diverse internet text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f91564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}