{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1581db",
   "metadata": {},
   "source": [
    "# Causal Attention: Teaching Models to Respect Time\n",
    "\n",
    "In the previous notebook, we learned about **Self-Attention with trainable weights** — how words can learn to pay attention to each other using Query, Key, and Value transformations.\n",
    "\n",
    "But there's a **critical problem** for language models: in standard self-attention, every word can see every other word, including words that come **after** it!\n",
    "\n",
    "## Why is This a Problem?\n",
    "\n",
    "Imagine you're training a model to predict the next word:\n",
    "\n",
    "```\n",
    "\"Your journey starts with one ____\"\n",
    "```\n",
    "\n",
    "If the word \"starts\" can see the word \"step\" (which comes after it), the model is **cheating**! It already knows what comes next.\n",
    "\n",
    "**During training:** The model would learn to just copy future words instead of actually learning language patterns.\n",
    "\n",
    "**During generation:** There ARE no future words yet — the model needs to generate them one by one!\n",
    "\n",
    "---\n",
    "\n",
    "## What is Causal Attention?\n",
    "\n",
    "**Causal** means \"respecting cause and effect\" — things in the past can affect the future, but not vice versa.\n",
    "\n",
    "```\n",
    "Standard Self-Attention:          Causal (Masked) Attention:\n",
    "                                  \n",
    "\"Your\"    sees: Your, journey,    \"Your\"    sees: Your ✓\n",
    "                starts, with,                      \n",
    "                one, step                          \n",
    "                                  \n",
    "\"journey\" sees: Your, journey,    \"journey\" sees: Your ✓, journey ✓\n",
    "                starts, with,     \n",
    "                one, step         \n",
    "                                  \n",
    "\"starts\"  sees: Your, journey,    \"starts\"  sees: Your ✓, journey ✓, starts ✓\n",
    "                starts, with,     \n",
    "                one, step         \n",
    "                                  \n",
    "... and so on                     ... each word only sees itself and previous words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## The Key Idea: Masking\n",
    "\n",
    "We'll use a **mask** to hide future tokens. The mask is a triangular matrix:\n",
    "\n",
    "```\n",
    "         Your  journey  starts  with  one  step\n",
    "Your     [1      0        0      0     0    0  ]  ← \"Your\" only sees itself\n",
    "journey  [1      1        0      0     0    0  ]  ← \"journey\" sees Your + itself\n",
    "starts   [1      1        1      0     0    0  ]  ← \"starts\" sees first 3 words\n",
    "with     [1      1        1      1     0    0  ]  ← ... and so on\n",
    "one      [1      1        1      1     1    0  ]\n",
    "step     [1      1        1      1     1    1  ]  ← \"step\" sees everything\n",
    "```\n",
    "\n",
    "**1 = can see, 0 = cannot see (masked)**\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Learn in This Notebook\n",
    "\n",
    "1. **Why masking is necessary** for autoregressive language models\n",
    "2. **Simple masking approach** — multiply by a triangular mask\n",
    "3. **The problem with simple masking** — broken probability distributions\n",
    "4. **Efficient masking** — using `-inf` before softmax\n",
    "5. **Dropout regularization** — preventing overfitting in attention\n",
    "6. **Batched processing** — handling multiple sequences at once\n",
    "7. **Complete CausalAttention class** — production-ready implementation\n",
    "\n",
    "Let's build this step by step!\n",
    "\n",
    "---\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c2777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bycuszhtj",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Our Input Embeddings\n",
    "\n",
    "We'll use the same sentence from our previous notebooks:\n",
    "\n",
    "**\"Your journey starts with one step\"**\n",
    "\n",
    "Each word is represented as a 3-dimensional embedding vector. Think of these as coordinates in a \"meaning space\" where similar words are close together.\n",
    "\n",
    "```\n",
    "Word      Position    Embedding Vector         What it might encode\n",
    "────      ────────    ────────────────         ────────────────────\n",
    "\"Your\"       x¹       [0.43, 0.15, 0.89]      possessive, personal\n",
    "\"journey\"    x²       [0.55, 0.87, 0.66]      noun, abstract concept\n",
    "\"starts\"     x³       [0.57, 0.85, 0.64]      verb, beginning action\n",
    "\"with\"       x⁴       [0.22, 0.58, 0.33]      preposition, connector\n",
    "\"one\"        x⁵       [0.77, 0.25, 0.10]      number, singular\n",
    "\"step\"       x⁶       [0.05, 0.80, 0.55]      noun, concrete action\n",
    "```\n",
    "\n",
    "**Note:** In a real model, these embeddings would be learned, not hand-crafted. But using fixed values helps us trace the computations exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb3bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "print(x_2)\n",
    "print(d_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kl31koucvbj",
   "metadata": {},
   "source": [
    "## Step 2: Recap — Self-Attention from Previous Notebook\n",
    "\n",
    "Before we add masking, let's quickly bring in the `SelfAttention_v2` class we built before. This gives us the foundation to build upon.\n",
    "\n",
    "**Quick Reminder of How Self-Attention Works:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                     SELF-ATTENTION PIPELINE                         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│   Input Embeddings                                                   │\n",
    "│         │                                                            │\n",
    "│         ├──────────────────┬──────────────────┐                     │\n",
    "│         ▼                  ▼                  ▼                     │\n",
    "│    ┌─────────┐        ┌─────────┐        ┌─────────┐               │\n",
    "│    │× W_query│        │ × W_key │        │× W_value│               │\n",
    "│    └────┬────┘        └────┬────┘        └────┬────┘               │\n",
    "│         │                  │                  │                     │\n",
    "│         ▼                  ▼                  ▼                     │\n",
    "│      Queries             Keys              Values                   │\n",
    "│         │                  │                  │                     │\n",
    "│         └────────┬─────────┘                  │                     │\n",
    "│                  ▼                            │                     │\n",
    "│           Q × Kᵀ = Attention Scores           │                     │\n",
    "│                  │                            │                     │\n",
    "│                  ▼                            │                     │\n",
    "│      softmax(scores/√d_k) = Weights           │                     │\n",
    "│                  │                            │                     │\n",
    "│                  └────────────┬───────────────┘                     │\n",
    "│                               ▼                                     │\n",
    "│                    Weights × Values = Context                       │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "The key insight: **Right now, every word attends to every other word.** We need to fix this for language modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d8e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb60fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hwtch4ehl9b",
   "metadata": {},
   "source": [
    "## Step 3: The Problem — All Words See All Words\n",
    "\n",
    "Let's compute the attention weights to see the issue. We'll manually run through the self-attention steps:\n",
    "\n",
    "1. **Transform inputs → Queries, Keys** using learned weight matrices\n",
    "2. **Compute attention scores** = Queries × Keysᵀ (dot products measuring similarity)\n",
    "3. **Apply softmax** to get attention weights (probabilities that sum to 1)\n",
    "\n",
    "Watch the attention weights matrix — notice how **every position has non-zero weights for every other position**. This means each word is \"looking at\" all other words, including future ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8d2db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vh9fuvn0xt",
   "metadata": {},
   "source": [
    "### Understanding the Attention Weights Matrix\n",
    "\n",
    "Let's visualize what this matrix means:\n",
    "\n",
    "```\n",
    "              ──────────── Keys (what each word offers) ────────────\n",
    "              Your   journey  starts   with    one    step\n",
    "Queries   Your    [0.19    0.16     0.17    0.16    0.17   0.15]  ← attends to ALL\n",
    "(what     journey [0.20    0.17     0.17    0.15    0.17   0.15]  ← attends to ALL\n",
    "each      starts  [0.20    0.17     0.17    0.15    0.17   0.15]  ← attends to ALL\n",
    "word      with    [0.19    0.17     0.17    0.16    0.17   0.16]  ← attends to ALL\n",
    "asks)     one     [0.18    0.17     0.17    0.16    0.17   0.16]  ← attends to ALL\n",
    "          step    [0.19    0.17     0.17    0.15    0.17   0.15]  ← attends to ALL\n",
    "```\n",
    "\n",
    "**The Problem:** Look at row 1 (\"Your\") — it has non-zero attention to \"journey\", \"starts\", \"with\", \"one\", and \"step\". But when generating \"Your\", we shouldn't know about ANY future words!\n",
    "\n",
    "**Each row should only have non-zero values up to and including that position.**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: First Attempt — Simple Masking with Multiplication\n",
    "\n",
    "Our first approach: create a **lower triangular matrix** of ones and zeros, then **multiply** it with the attention weights to zero out the upper triangle (future positions).\n",
    "\n",
    "`torch.tril()` = \"triangular lower\" — gives us a matrix with 1s on and below the diagonal, 0s above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf6ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5o4zrxqgt",
   "metadata": {},
   "source": [
    "### Visualizing the Mask\n",
    "\n",
    "```\n",
    "         Your  journey  starts  with  one  step\n",
    "Your     [1      0        0      0     0    0  ]   ← Can only see itself\n",
    "journey  [1      1        0      0     0    0  ]   ← Can see Your + itself\n",
    "starts   [1      1        1      0     0    0  ]   ← Can see first 3 words\n",
    "with     [1      1        1      1     0    0  ]   ← Can see first 4 words\n",
    "one      [1      1        1      1     1    0  ]   ← Can see first 5 words\n",
    "step     [1      1        1      1     1    1  ]   ← Can see everything\n",
    "```\n",
    "\n",
    "Now let's multiply: `attention_weights × mask`. Wherever the mask is 0, the attention weight becomes 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819f9db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rh465krcfz9",
   "metadata": {},
   "source": [
    "### The Problem: Rows Don't Sum to 1 Anymore!\n",
    "\n",
    "Look at the first row: `[0.19, 0.0, 0.0, 0.0, 0.0, 0.0]`\n",
    "\n",
    "**This sums to only 0.19, not 1.0!**\n",
    "\n",
    "Attention weights are supposed to be a **probability distribution** — they must sum to 1 so we compute a proper weighted average of values.\n",
    "\n",
    "```\n",
    "Before masking:  0.19 + 0.16 + 0.17 + 0.16 + 0.17 + 0.15 = 1.00 ✓\n",
    "After masking:   0.19 + 0.00 + 0.00 + 0.00 + 0.00 + 0.00 = 0.19 ✗\n",
    "```\n",
    "\n",
    "**Simple fix:** Divide each row by its sum to renormalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730da4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aoet7dkv7bh",
   "metadata": {},
   "source": [
    "### Success! Now Rows Sum to 1\n",
    "\n",
    "```\n",
    "Row 1 (Your):    [1.00, 0.00, 0.00, 0.00, 0.00, 0.00]  sum = 1.00 ✓\n",
    "Row 2 (journey): [0.55, 0.45, 0.00, 0.00, 0.00, 0.00]  sum = 1.00 ✓\n",
    "Row 3 (starts):  [0.38, 0.31, 0.31, 0.00, 0.00, 0.00]  sum = 1.00 ✓\n",
    "...\n",
    "```\n",
    "\n",
    "**This works, but there's a more elegant approach!**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: The Better Way — Masking with -∞ Before Softmax\n",
    "\n",
    "Instead of:\n",
    "1. Apply softmax → get weights\n",
    "2. Multiply by mask → zero out future\n",
    "3. Renormalize → divide by row sum\n",
    "\n",
    "We can do:\n",
    "1. Add -∞ to attention scores where we want to mask\n",
    "2. Apply softmax → **automatically get zeros!**\n",
    "\n",
    "**Why does this work?**\n",
    "\n",
    "```\n",
    "softmax(x) = e^x / Σe^x\n",
    "\n",
    "When x = -∞:\n",
    "- e^(-∞) = 0\n",
    "- So those positions contribute 0 to the numerator AND denominator\n",
    "- Result: they naturally become 0, and remaining values sum to 1!\n",
    "```\n",
    "\n",
    "**Much cleaner!** One operation instead of three, and mathematically equivalent.\n",
    "\n",
    "### Creating the Upper Triangular Mask\n",
    "\n",
    "`torch.triu()` = \"triangular upper\" — gives us 1s above the diagonal (positions to mask).\n",
    "\n",
    "We then use `masked_fill()` to replace those positions with -∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df1dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mo4p10yq3qf",
   "metadata": {},
   "source": [
    "### Understanding the Masked Scores\n",
    "\n",
    "Look at the output:\n",
    "```\n",
    "Row 1: [0.29,  -inf,  -inf,  -inf,  -inf,  -inf]  \"Your\" can only see itself\n",
    "Row 2: [0.47,  0.17,  -inf,  -inf,  -inf,  -inf]  \"journey\" sees Your + itself\n",
    "Row 3: [0.46,  0.17,  0.17,  -inf,  -inf,  -inf]  \"starts\" sees first 3\n",
    "...\n",
    "```\n",
    "\n",
    "The -∞ values will become 0 after softmax. Now let's apply softmax to get our final attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04727b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jqwqd0hxcec",
   "metadata": {},
   "source": [
    "### Perfect! The Causal Attention Weights\n",
    "\n",
    "```\n",
    "              Your   journey  starts   with    one    step\n",
    "Your         [1.00    0.00     0.00    0.00   0.00   0.00]  ← only self\n",
    "journey      [0.55    0.45     0.00    0.00   0.00   0.00]  ← past + self\n",
    "starts       [0.38    0.31     0.31    0.00   0.00   0.00]  ← past + self\n",
    "with         [0.28    0.25     0.25    0.23   0.00   0.00]  ← past + self\n",
    "one          [0.22    0.20     0.20    0.19   0.20   0.00]  ← past + self\n",
    "step         [0.19    0.17     0.17    0.15   0.17   0.15]  ← all (last word)\n",
    "```\n",
    "\n",
    "**Notice:**\n",
    "- Row 1 is `[1, 0, 0, 0, 0, 0]` — \"Your\" puts 100% attention on itself\n",
    "- Each subsequent row has more non-zero entries\n",
    "- Row 6 is identical to standard self-attention (last word sees everything)\n",
    "- **All rows sum to 1.0** — proper probability distributions!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Adding Dropout for Regularization\n",
    "\n",
    "**What is Dropout?**\n",
    "\n",
    "Dropout is a technique to prevent **overfitting** (when a model memorizes training data instead of learning patterns). During training, we randomly \"drop\" (set to zero) some values with probability `p`.\n",
    "\n",
    "**Why use dropout on attention weights?**\n",
    "\n",
    "Without dropout, a model might learn to rely too heavily on specific attention patterns. Dropout forces the model to be robust — it can't depend on any single connection always being there.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "```\n",
    "Original:  [0.2, 0.3, 0.1, 0.4]\n",
    "                  ↓\n",
    "           Randomly zero some (p=0.5)\n",
    "                  ↓\n",
    "Dropped:   [0.4, 0.0, 0.2, 0.0]   ← zeros inserted\n",
    "                  ↓\n",
    "           Scale up remaining (×2)\n",
    "                  ↓\n",
    "Final:     [0.4, 0.0, 0.2, 0.0]   ← keeps expected value same\n",
    "```\n",
    "\n",
    "**The 2× scaling:** When we drop 50% of values, we double the remaining ones. This way, the average output stays the same whether dropout is on (training) or off (inference).\n",
    "\n",
    "Let's see dropout in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59172639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1zs5ixfsl6",
   "metadata": {},
   "source": [
    "### Understanding the Dropout Output\n",
    "\n",
    "We applied 50% dropout to a matrix of 1s:\n",
    "\n",
    "```\n",
    "Original: all 1s\n",
    "After:    2s and 0s randomly placed\n",
    "```\n",
    "\n",
    "**Why 2s instead of 1s?** \n",
    "\n",
    "With 50% dropout:\n",
    "- ~Half the values become 0\n",
    "- The remaining half are scaled by `1/(1-0.5) = 2`\n",
    "- This keeps the expected value the same!\n",
    "\n",
    "```\n",
    "Expected value before: 1.0 × 1.0 = 1.0\n",
    "Expected value after:  (0.5 × 0) + (0.5 × 2) = 1.0  ✓\n",
    "```\n",
    "\n",
    "Now let's apply dropout to our causal attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ffdb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdqzg9rjt58",
   "metadata": {},
   "source": [
    "### Dropout Applied to Attention Weights\n",
    "\n",
    "Notice:\n",
    "- Some attention weights are now 0 (dropped)\n",
    "- Others are doubled (scaled up)\n",
    "- Row 2 is all zeros! The model can't use \"journey\"'s attention for this forward pass\n",
    "- This forces the model to not rely too heavily on any single attention pattern\n",
    "\n",
    "**Important:** Dropout is only active during training! During inference/generation, we disable it so the model uses all learned patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Handling Batched Inputs\n",
    "\n",
    "In practice, we don't process one sentence at a time. We process **batches** of sentences together for efficiency (GPUs are optimized for parallel operations).\n",
    "\n",
    "**Batched tensor shape:** `[batch_size, sequence_length, embedding_dim]`\n",
    "\n",
    "```\n",
    "Single input:     [6, 3]         ← 6 tokens, 3-dim embeddings\n",
    "Batched input:    [2, 6, 3]      ← 2 sequences, each with 6 tokens, 3-dim embeddings\n",
    "                   ↑  ↑  ↑\n",
    "                batch seq embed\n",
    "```\n",
    "\n",
    "Let's create a batch by stacking our input twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd609d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z7n138b12hb",
   "metadata": {},
   "source": [
    "We now have a batch of 2 sequences, each with 6 tokens of 3-dimensional embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: The Complete CausalAttention Class\n",
    "\n",
    "Now let's put everything together into a production-ready PyTorch module!\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **`__init__`**: Set up the module\n",
    "   - Create Q, K, V linear layers\n",
    "   - Create dropout layer\n",
    "   - Register the causal mask as a **buffer** (not a parameter — it's not learned)\n",
    "\n",
    "2. **`forward`**: Compute causal attention\n",
    "   - Transform inputs → Q, K, V\n",
    "   - Compute attention scores\n",
    "   - Apply causal mask (set future positions to -∞)\n",
    "   - Apply softmax\n",
    "   - Apply dropout\n",
    "   - Compute weighted sum of values\n",
    "\n",
    "### Why `register_buffer` for the mask?\n",
    "\n",
    "```python\n",
    "self.register_buffer('mask', torch.triu(...))\n",
    "```\n",
    "\n",
    "- The mask is a **constant** — it doesn't change during training\n",
    "- `register_buffer` makes it part of the module's state (saved/loaded with model)\n",
    "- It moves to GPU automatically when you call `model.to('cuda')`\n",
    "- But it's NOT a learnable parameter — no gradients computed for it\n",
    "\n",
    "### Handling Variable Sequence Lengths\n",
    "\n",
    "```python\n",
    "self.mask.bool()[:num_tokens, :num_tokens]\n",
    "```\n",
    "\n",
    "The mask is created for the maximum context length, but we slice it to match the actual sequence length. This allows the same module to handle sequences of different lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315acb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "           'mask',\n",
    "           torch.triu(torch.ones(context_length, context_length),\n",
    "           diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)   \n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08yatlo76pi",
   "metadata": {},
   "source": [
    "### Understanding the Code Line by Line\n",
    "\n",
    "```python\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        # The three learnable projections\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Pre-computed causal mask (upper triangular = positions to mask)\n",
    "        # register_buffer: part of state but not a learnable parameter\n",
    "        self.register_buffer(\n",
    "           'mask',\n",
    "           torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # Unpack batch dimensions\n",
    "        \n",
    "        # Step 1: Project to Q, K, V\n",
    "        keys = self.W_key(x)       # [batch, seq, d_out]\n",
    "        queries = self.W_query(x)  # [batch, seq, d_out]\n",
    "        values = self.W_value(x)   # [batch, seq, d_out]\n",
    "\n",
    "        # Step 2: Compute attention scores\n",
    "        # Note: transpose(1,2) swaps seq and d_out dims for batched matmul\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # [batch, seq, seq]\n",
    "        \n",
    "        # Step 3: Apply causal mask (set future to -inf)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        \n",
    "        # Step 4: Softmax + scaling\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        # Step 5: Apply dropout (only during training)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Step 6: Weighted sum of values\n",
    "        context_vec = attn_weights @ values  # [batch, seq, d_out]\n",
    "        return context_vec\n",
    "```\n",
    "\n",
    "Now let's test it with our batched input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d506c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4r0p2no4j1t",
   "metadata": {},
   "source": [
    "### Output Shape Explained\n",
    "\n",
    "```\n",
    "Input:  [2, 6, 3]   →   Output: [2, 6, 2]\n",
    "         ↑  ↑  ↑                 ↑  ↑  ↑\n",
    "      batch seq d_in          batch seq d_out\n",
    "```\n",
    "\n",
    "- **Batch size (2):** Preserved — we still have 2 sequences\n",
    "- **Sequence length (6):** Preserved — still 6 positions\n",
    "- **Embedding dimension:** Changed from `d_in=3` to `d_out=2` (the projection dimension)\n",
    "\n",
    "Each output vector is a **context-aware representation** that only incorporates information from previous positions (thanks to causal masking)!\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "### The Journey from Self-Attention to Causal Attention\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    CAUSAL ATTENTION PIPELINE                        │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│   Input: [batch, seq_len, d_in]                                     │\n",
    "│              │                                                       │\n",
    "│              ├─────────────────┬─────────────────┐                  │\n",
    "│              ▼                 ▼                 ▼                  │\n",
    "│         ┌────────┐        ┌────────┐        ┌────────┐             │\n",
    "│         │W_query │        │ W_key  │        │W_value │             │\n",
    "│         └───┬────┘        └───┬────┘        └───┬────┘             │\n",
    "│             │                 │                 │                   │\n",
    "│             ▼                 ▼                 ▼                   │\n",
    "│          Queries            Keys             Values                 │\n",
    "│             │                 │                 │                   │\n",
    "│             └────────┬────────┘                 │                   │\n",
    "│                      ▼                          │                   │\n",
    "│               Q × Kᵀ = Scores                   │                   │\n",
    "│                      │                          │                   │\n",
    "│                      ▼                          │                   │\n",
    "│           ┌──────────────────┐                  │                   │\n",
    "│           │  CAUSAL MASK     │  ← NEW!          │                   │\n",
    "│           │  (set future     │                  │                   │\n",
    "│           │   to -∞)         │                  │                   │\n",
    "│           └────────┬─────────┘                  │                   │\n",
    "│                    ▼                            │                   │\n",
    "│             Softmax / √d_k                      │                   │\n",
    "│                    │                            │                   │\n",
    "│                    ▼                            │                   │\n",
    "│           ┌──────────────────┐                  │                   │\n",
    "│           │    DROPOUT       │  ← NEW!          │                   │\n",
    "│           └────────┬─────────┘                  │                   │\n",
    "│                    │                            │                   │\n",
    "│                    └──────────────┬─────────────┘                   │\n",
    "│                                   ▼                                 │\n",
    "│                           Weights × Values                          │\n",
    "│                                   │                                 │\n",
    "│                                   ▼                                 │\n",
    "│                   Output: [batch, seq_len, d_out]                   │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | What We Learned |\n",
    "|---------|-----------------|\n",
    "| **Causal Masking** | Prevents tokens from attending to future positions (essential for autoregressive generation) |\n",
    "| **-∞ Trick** | Setting scores to -∞ before softmax elegantly produces zeros while maintaining proper probabilities |\n",
    "| **Dropout** | Randomly drops attention connections during training to prevent overfitting |\n",
    "| **Batching** | Processing multiple sequences together with shape `[batch, seq, embed]` |\n",
    "| **register_buffer** | Storing non-learnable constants that travel with the model |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook, we'll learn about **Multi-Head Attention** — running multiple attention \"heads\" in parallel, each learning to focus on different aspects of the input. This is what makes transformers so powerful!\n",
    "\n",
    "```\n",
    "Single Head:     One attention pattern\n",
    "Multi-Head:      Multiple attention patterns combined\n",
    "                 → Head 1 might focus on syntax\n",
    "                 → Head 2 might focus on semantics\n",
    "                 → Head 3 might focus on position\n",
    "                 → etc.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
