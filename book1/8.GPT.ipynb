{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "h5gk17i61am",
   "metadata": {},
   "source": [
    "# Chapter 8: Building the Complete GPT Model\n",
    "\n",
    "## From Transformer Blocks to a Full Language Model\n",
    "\n",
    "In the previous chapters, we built all the individual components:\n",
    "- **Chapter 4:** Causal Attention (masking future tokens)\n",
    "- **Chapter 5:** Multi-Head Attention (parallel attention patterns)\n",
    "- **Chapter 6:** LayerNorm, GELU, FeedForward\n",
    "- **Chapter 7:** TransformerBlock (combining everything)\n",
    "\n",
    "Now it's time to **stack these blocks** and add the missing pieces to create a complete GPT model!\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         GPT-2 (124M)                                â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  Token IDs â”€â”€â–º Token Embedding â”€â”€â”                                  â”‚\n",
    "â”‚                                  â”œâ”€â”€â–º (+) â”€â”€â–º Dropout               â”‚\n",
    "â”‚  Positions â”€â”€â–º Pos Embedding â”€â”€â”€â”€â”˜              â”‚                   â”‚\n",
    "â”‚                                                 â–¼                   â”‚\n",
    "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚                                    â”‚ TransformerBlock 1  â”‚          â”‚\n",
    "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                               â–¼                     â”‚\n",
    "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚                                    â”‚ TransformerBlock 2  â”‚          â”‚\n",
    "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                               â–¼                     â”‚\n",
    "â”‚                                             ...                     â”‚\n",
    "â”‚                                               â–¼                     â”‚\n",
    "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚                                    â”‚ TransformerBlock 12 â”‚          â”‚\n",
    "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                               â–¼                     â”‚\n",
    "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚                                    â”‚     LayerNorm       â”‚          â”‚\n",
    "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                               â–¼                     â”‚\n",
    "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚                                    â”‚   Output Linear     â”‚â”€â”€â–º Logitsâ”‚\n",
    "â”‚                                    â”‚   (768 â†’ 50257)     â”‚          â”‚\n",
    "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| Token Embedding | Convert token IDs â†’ dense vectors |\n",
    "| Position Embedding | Add position information |\n",
    "| 12Ã— TransformerBlock | The core processing layers |\n",
    "| Final LayerNorm | Stabilize before output |\n",
    "| Output Head | Project to vocabulary logits |\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d22ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ugamj5w8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Importing Our Building Blocks\n",
    "\n",
    "First, we import all the components we built in previous chapters. These are the \"Lego bricks\" that make up GPT:\n",
    "\n",
    "| Component | What It Does | Built In |\n",
    "|-----------|-------------|----------|\n",
    "| `MultiHeadAttention` | Tokens attend to each other (12 heads in parallel) | Chapter 5 |\n",
    "| `LayerNorm` | Normalizes activations for stable training | Chapter 6 |\n",
    "| `GELU` | Smooth activation function | Chapter 6 |\n",
    "| `FeedForward` | Two-layer MLP (expand 4x, then contract) | Chapter 6 |\n",
    "| `TransformerBlock` | Combines attention + FFN with residuals | Chapter 7 |\n",
    "\n",
    "We'll copy these classes here so this notebook is self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb09ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea8861d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16164a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcbc34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "477a8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pau1nop8v3k",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: GPT-2 Configuration\n",
    "\n",
    "Before building the model, we define all the hyperparameters. GPT-2 comes in different sizes:\n",
    "\n",
    "| Model | Parameters | Layers | Heads | Embedding Dim |\n",
    "|-------|-----------|--------|-------|---------------|\n",
    "| GPT-2 Small | **124M** | 12 | 12 | 768 |\n",
    "| GPT-2 Medium | 355M | 24 | 16 | 1024 |\n",
    "| GPT-2 Large | 774M | 36 | 20 | 1280 |\n",
    "| GPT-2 XL | 1.5B | 48 | 25 | 1600 |\n",
    "\n",
    "We'll implement the **124M** version - small enough to train on a single GPU, but large enough to generate coherent text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bmbdh9kv",
   "metadata": {},
   "source": [
    "### Configuration Parameters Explained\n",
    "\n",
    "| Parameter | Value | Meaning |\n",
    "|-----------|-------|---------|\n",
    "| `vocab_size` | 50,257 | GPT-2's BPE vocabulary (50k tokens + special tokens) |\n",
    "| `context_length` | 1,024 | Maximum sequence length the model can process |\n",
    "| `emb_dim` | 768 | Size of token embeddings (d_model) |\n",
    "| `n_heads` | 12 | Number of attention heads (768 Ã· 12 = 64 per head) |\n",
    "| `n_layers` | 12 | Number of TransformerBlocks stacked |\n",
    "| `drop_rate` | 0.1 | 10% dropout for regularization |\n",
    "| `qkv_bias` | False | No bias in Q, K, V projections (GPT-2 choice) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a24d0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1w8ud7zc4pc",
   "metadata": {},
   "source": [
    "### Quick Test: Verify TransformerBlock Works\n",
    "\n",
    "Before building the full model, let's verify our TransformerBlock works with the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46e51563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ydd6e7p9juo",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Complete GPTModel Class\n",
    "\n",
    "Now we build the **full GPT model** by combining:\n",
    "\n",
    "1. **Token Embeddings** - Lookup table: token ID â†’ vector\n",
    "2. **Position Embeddings** - Lookup table: position â†’ vector  \n",
    "3. **Embedding Dropout** - Regularization\n",
    "4. **12 TransformerBlocks** - The core of GPT\n",
    "5. **Final LayerNorm** - Stabilize outputs\n",
    "6. **Output Head** - Project to vocabulary logits\n",
    "\n",
    "### The Forward Pass Visualized\n",
    "\n",
    "```\n",
    "        Token IDs: [15496, 11, 314, 716]  (\"Hello, I am\")\n",
    "                           â”‚\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â–¼                               â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ tok_emb     â”‚                 â”‚ pos_emb     â”‚\n",
    "    â”‚ (50257Ã—768) â”‚                 â”‚ (1024Ã—768)  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚                               â”‚\n",
    "           â–¼                               â–¼\n",
    "    (batch, seq, 768)              (seq, 768)\n",
    "           â”‚                               â”‚\n",
    "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚ ADD (+)\n",
    "                           â–¼\n",
    "                   (batch, seq, 768)\n",
    "                           â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  drop_emb   â”‚  Dropout(0.1)\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   12Ã— TransformerBlock   â”‚\n",
    "              â”‚   (attention + FFN)      â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ final_norm  â”‚  LayerNorm\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  out_head   â”‚  Linear(768 â†’ 50257)\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "            Logits: (batch, seq, 50257)\n",
    "            \n",
    "            For each position, we get a score for\n",
    "            every token in the vocabulary!\n",
    "```\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "The model outputs **logits** (unnormalized scores) for each vocabulary token:\n",
    "- Shape: `(batch_size, sequence_length, vocab_size)`\n",
    "- To get probabilities: `softmax(logits, dim=-1)`\n",
    "- To get predictions: `argmax(logits, dim=-1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e3077ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2b0455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y24khfdn3pl",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Testing with Real Data\n",
    "\n",
    "Now let's test our GPT model with actual text! We'll use the dataloader from Chapter 1 to create proper training batches.\n",
    "\n",
    "### Redefine the Config (for clarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0gqofq48b1zq",
   "metadata": {},
   "source": [
    "### The Data Pipeline for Language Modeling\n",
    "\n",
    "GPT is trained to predict the **next token**. The dataloader creates input-target pairs:\n",
    "\n",
    "```\n",
    "\"Frankenstein\" text\n",
    "        â”‚\n",
    "        â–¼ tokenize (BPE)\n",
    "   [171, 119, 123, 464, 4935, ...]  â—„â”€â”€ Token IDs\n",
    "        â”‚\n",
    "        â–¼ sliding window\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Input:  [tok_0, tok_1, ..., tok_255]â”‚\n",
    "   â”‚ Target: [tok_1, tok_2, ..., tok_256]â”‚  â—„â”€â”€ Shifted by 1!\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚\n",
    "        â–¼ DataLoader batches\n",
    "   Shape: (batch_size, seq_len) = (2, 256)\n",
    "        â”‚\n",
    "        â–¼ GPT Model\n",
    "   Shape: (2, 256, 50257)  â—„â”€â”€ Logits over vocabulary\n",
    "```\n",
    "\n",
    "**Key Insight:** The target is always the input shifted by one position. This is how GPT learns to predict \"what comes next\"!\n",
    "\n",
    "### Step 1: Download Training Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0954ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 'pg84.txt'\n",
      "Text length: 438807 characters\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import requests\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
    "filename = \"pg84.txt\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Successfully downloaded '{filename}'\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading the file: {e}\")\n",
    "\n",
    "# Load the text\n",
    "with open(\"pg84.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Text length: {len(raw_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7sawjdmnan",
   "metadata": {},
   "source": [
    "### Step 2: Define Dataset and DataLoader\n",
    "\n",
    "The `GPTDatasetV1` class creates training examples using a **sliding window**:\n",
    "\n",
    "```\n",
    "Text: \"The cat sat on the mat\"\n",
    "Tokens: [464, 3797, 3332, 319, 262, 2603]\n",
    "\n",
    "With max_length=4, stride=4 (no overlap):\n",
    "\n",
    "Example 1:\n",
    "  Input:  [464, 3797, 3332, 319]   \"The cat sat on\"\n",
    "  Target: [3797, 3332, 319, 262]   \"cat sat on the\"\n",
    "\n",
    "Example 2:  \n",
    "  Input:  [262, 2603, ...]          \"the mat ...\"\n",
    "  Target: [2603, ...]               \"mat ...\"\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `max_length`: How many tokens per training example\n",
    "- `stride`: How far to slide between examples (stride=max_length means no overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdeeeomawj",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    \"\"\"Creates input-target pairs using sliding window for next-token prediction.\"\"\"\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \"\"\"Create a DataLoader with GPT-2 BPE tokenization.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cht8rlyrwng",
   "metadata": {},
   "source": [
    "### Step 3: Create DataLoader and Inspect a Batch\n",
    "\n",
    "Now let's create a dataloader and look at what we get:\n",
    "\n",
    "**Important requirements for GPT input:**\n",
    "- Must be **2D tensor**: `(batch_size, sequence_length)`\n",
    "- Must be **integer token IDs**, not floats\n",
    "- Sequence length must be â‰¤ `context_length` (1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "\n",
      "First 10 tokens of input[0]: tensor([  171,   119,   123,   464,  4935, 20336, 46566,   286, 45738,    26])\n",
      "First 10 tokens of target[0]: tensor([  119,   123,   464,  4935, 20336, 46566,   286, 45738,    26,  1471])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataloader with appropriate settings for GPT-2\n",
    "# max_length should be <= context_length (1024 for GPT-2)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=2,       # Small batch for testing\n",
    "    max_length=256,     # Sequence length (must be <= 1024)\n",
    "    stride=256,         # Non-overlapping chunks\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get a batch\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)   # (batch_size, seq_len) = (2, 256)\n",
    "print(\"Target shape:\", targets.shape)\n",
    "print(\"\\nFirst 10 tokens of input[0]:\", inputs[0][:10])\n",
    "print(\"First 10 tokens of target[0]:\", targets[0][:10])  # Shifted by 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mo152thxs7",
   "metadata": {},
   "source": [
    "### Step 4: Pass Data Through the GPT Model\n",
    "\n",
    "Now the exciting part - let's feed our batch through the model!\n",
    "\n",
    "```\n",
    "Input:  (2, 256)      â”€â”€â–º GPT Model â”€â”€â–º  Output: (2, 256, 50257)\n",
    "        â†‘                                         â†‘\n",
    "   batch of 2                              logits for each\n",
    "   sequences,                              position over\n",
    "   256 tokens each                         50,257 vocab tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rs6rs37hvuf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 256])\n",
      "Output shape: torch.Size([2, 256, 50257])\n",
      "\n",
      "Predicted tokens shape: torch.Size([2, 256])\n",
      "First 10 predicted tokens: tensor([ 7733, 35073, 42775, 44367, 29289,  9580, 39155, 38019, 34764, 23338])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Forward pass - inputs are token IDs with shape (batch, seq_len)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)      # (2, 256) - token IDs\n",
    "print(\"Output shape:\", logits.shape)     # (2, 256, 50257) - logits for each position\n",
    "\n",
    "# The output is logits over the vocabulary for each position\n",
    "# To get predicted tokens, we take argmax\n",
    "predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "print(\"\\nPredicted tokens shape:\", predicted_tokens.shape)\n",
    "print(\"First 10 predicted tokens:\", predicted_tokens[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssrg0vqwalr",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Understanding Model Size\n",
    "\n",
    "Let's count the parameters to understand where the \"124M\" comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c91479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x8af7y991wp",
   "metadata": {},
   "source": [
    "### Total Parameter Count\n",
    "\n",
    "Let's see exactly how many parameters our model has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b0d3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alsml6f5ttn",
   "metadata": {},
   "source": [
    "### Weight Tying: A Clever Trick\n",
    "\n",
    "Notice something interesting - the token embedding and output head have the **same shape**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac19b20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dw173kahtk9",
   "metadata": {},
   "source": [
    "In the original GPT-2, these weights are **tied** (shared), saving ~38M parameters!\n",
    "\n",
    "```\n",
    "Token Embedding: (50257, 768) = 38.6M parameters\n",
    "Output Head:     (50257, 768) = 38.6M parameters\n",
    "\n",
    "Without tying: 38.6M + 38.6M = 77.2M\n",
    "With tying:    38.6M (shared) = 38.6M  â—„â”€â”€ Saves half!\n",
    "```\n",
    "\n",
    "This is called **weight tying** - the intuition is that the embedding of a word and the output prediction for that word should use the same representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bziveix7hfp",
   "metadata": {},
   "source": [
    "### Memory Footprint\n",
    "\n",
    "Each parameter is stored as a 32-bit float (4 bytes). Let's calculate the model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a39cf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kc1r1izysa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Text Generation\n",
    "\n",
    "Now for the fun part - let's make GPT generate text! Even though it's untrained (random weights), we can still see the generation process in action.\n",
    "\n",
    "### How Text Generation Works (Autoregressive Decoding)\n",
    "\n",
    "GPT generates text **one token at a time**, feeding its own output back as input:\n",
    "\n",
    "```\n",
    "Step 1: \"Hello\" â”€â”€â–º GPT â”€â”€â–º predict \",\" \n",
    "Step 2: \"Hello,\" â”€â”€â–º GPT â”€â”€â–º predict \" I\"\n",
    "Step 3: \"Hello, I\" â”€â”€â–º GPT â”€â”€â–º predict \" am\"\n",
    "Step 4: \"Hello, I am\" â”€â”€â–º GPT â”€â”€â–º predict \" a\"\n",
    "...and so on\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  Autoregressive Loop                    â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚   Input: [tok_0, tok_1, ..., tok_n]                    â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚              â–¼                                          â”‚\n",
    "â”‚           GPT Model                                     â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚              â–¼                                          â”‚\n",
    "â”‚   Logits: (1, n, 50257)                                â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚              â–¼ take last position only                  â”‚\n",
    "â”‚   logits[:, -1, :] â†’ (1, 50257)                        â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚              â–¼ softmax â†’ probabilities                  â”‚\n",
    "â”‚              â–¼ argmax â†’ most likely token               â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚   idx_next = predicted token ID                        â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚              â–¼ append to input                          â”‚\n",
    "â”‚   Input: [tok_0, tok_1, ..., tok_n, idx_next]          â”‚\n",
    "â”‚              â”‚                                          â”‚\n",
    "â”‚              â””â”€â”€â”€â”€â”€â”€ repeat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Simple Generation Function\n",
    "\n",
    "This function implements **greedy decoding** - always picking the most likely next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b39372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,\n",
    "                         max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qwce8qjux3s",
   "metadata": {},
   "source": [
    "### Prepare the Starting Prompt\n",
    "\n",
    "We tokenize a starting prompt and convert it to a tensor. This is the \"seed\" for generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd9fc9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8pb0r50exn",
   "metadata": {},
   "source": [
    "### Generate New Tokens\n",
    "\n",
    "Let's generate 6 new tokens starting from \"Hello, I am\":\n",
    "\n",
    "**Note:** Since the model is untrained (random weights), the output will be **gibberish**! After training, it would generate coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed9ecb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zh47qo5itt",
   "metadata": {},
   "source": [
    "### Decode Back to Text\n",
    "\n",
    "Convert the token IDs back to human-readable text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31cd99e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224afe9",
   "metadata": {},
   "source": [
    "### Comparing Input, Prediction, and Target\n",
    "\n",
    "Let's visualize how the untrained model's predictions compare to what we actually want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r2edqqeqhc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INPUT TEXT (first 200 chars):\n",
      "============================================================\n",
      "ï»¿The Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with alm\n",
      "\n",
      "============================================================\n",
      "MODEL PREDICTION (first 200 chars) - Random, untrained!\n",
      "============================================================\n",
      " gearnin canyon 349layer screw globalizationCHAR WARRANTÙ†diff Mastinding council undecided cyclists can arisesRoom triedichoie162 Exec Under Caval342 Canal Pinball darkest diff Pianoquestionolen Verno\n",
      "\n",
      "============================================================\n",
      "TARGET TEXT (first 200 chars) - What we want to predict\n",
      "============================================================\n",
      "ï¿½ï¿½The Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with al\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Decode input and predicted output\n",
    "input_text = tokenizer.decode(inputs[0].tolist())\n",
    "predicted_text = tokenizer.decode(predicted_tokens[0].tolist())\n",
    "target_text = tokenizer.decode(targets[0].tolist())\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INPUT TEXT (first 200 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(input_text[:200])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PREDICTION (first 200 chars) - Random, untrained!\")\n",
    "print(\"=\" * 60)\n",
    "print(predicted_text[:200])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET TEXT (first 200 chars) - What we want to predict\")\n",
    "print(\"=\" * 60)\n",
    "print(target_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mxauhphnqej",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "We assembled a complete **GPT-2 (124M)** model from scratch:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      GPTModel                              â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
    "â”‚  â”‚  tok_emb     â”‚  â”‚  pos_emb     â”‚  Embeddings           â”‚\n",
    "â”‚  â”‚ (50257Ã—768)  â”‚  â”‚ (1024Ã—768)   â”‚                       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                  â”‚ (+)                                     â”‚\n",
    "â”‚                  â–¼                                         â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚   drop_emb     â”‚  Dropout(0.1)                  â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                  â–¼                                         â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚ trf_blocks     â”‚  12Ã— TransformerBlock          â”‚\n",
    "â”‚         â”‚  (Sequential)  â”‚                                â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                  â–¼                                         â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚  final_norm    â”‚  LayerNorm(768)                â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                  â–¼                                         â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚   out_head     â”‚  Linear(768 â†’ 50257)           â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                  â–¼                                         â”‚\n",
    "â”‚            Logits (vocab scores)                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **GPT Architecture**:\n",
    "   - Token embeddings + Position embeddings (added together)\n",
    "   - Stack of TransformerBlocks (12 for GPT-2 Small)\n",
    "   - Final LayerNorm + Linear output head\n",
    "\n",
    "2. **Model Size (124M parameters)**:\n",
    "   - Embeddings: ~39M (token) + ~0.8M (position)\n",
    "   - TransformerBlocks: ~7M each Ã— 12 = ~85M\n",
    "   - Output head: ~39M (can be tied with token embedding)\n",
    "\n",
    "3. **Input/Output**:\n",
    "   - Input: Token IDs `(batch, seq_len)` - integers!\n",
    "   - Output: Logits `(batch, seq_len, vocab_size)` - scores for each token\n",
    "\n",
    "4. **Text Generation**:\n",
    "   - Autoregressive: generate one token at a time\n",
    "   - Feed output back as input\n",
    "   - Untrained = random gibberish\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "The model generates **random garbage** because it's untrained. In the next chapters, we'll:\n",
    "1. **Train** the model on text data\n",
    "2. **Load pre-trained weights** from OpenAI's GPT-2\n",
    "3. Implement better generation strategies (temperature, top-k, top-p)\n",
    "\n",
    "The architecture is complete - now we just need to teach it to speak! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
