{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d22ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lvf4rwpewk",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Building the Model Architecture\n",
    "\n",
    "First, we need to assemble all the building blocks of our GPT model. We've covered these in previous notebooks, so here we'll import/define them and focus on how they work together.\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "The **heart of the transformer**. Multi-head attention allows the model to:\n",
    "- Look at different positions in the sequence simultaneously\n",
    "- Learn different types of relationships (syntactic, semantic, etc.) via multiple \"heads\"\n",
    "- Use **causal masking** to prevent looking at future tokens (crucial for language modeling!)\n",
    "\n",
    "### How Multi-Head Attention Works:\n",
    "\n",
    "```\n",
    "Input: [batch, seq_len, d_model]\n",
    "         │\n",
    "         ├──→ Q = input @ W_query ──┐\n",
    "         ├──→ K = input @ W_key   ──┼──→ Split into num_heads\n",
    "         └──→ V = input @ W_value ──┘\n",
    "                                    │\n",
    "                    ┌───────────────┴───────────────┐\n",
    "                    │     For each head:            │\n",
    "                    │  1. attention = Q @ K.T       │\n",
    "                    │  2. mask future positions     │\n",
    "                    │  3. scale by √d_k             │\n",
    "                    │  4. softmax → weights         │\n",
    "                    │  5. output = weights @ V      │\n",
    "                    └───────────────┬───────────────┘\n",
    "                                    │\n",
    "                            Concatenate all heads\n",
    "                                    │\n",
    "                            Output projection\n",
    "                                    │\n",
    "Output: [batch, seq_len, d_model]\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "- `d_in`: Input dimension (embedding size)\n",
    "- `d_out`: Output dimension (usually same as d_in)\n",
    "- `num_heads`: Number of attention heads (d_out must be divisible by this)\n",
    "- `context_length`: Maximum sequence length (for creating the causal mask)\n",
    "- `dropout`: Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ze6l5m8eftb",
   "metadata": {},
   "source": [
    "# Pre-Training a GPT Model from Scratch\n",
    "\n",
    "This notebook walks through the complete process of **pre-training** a GPT-style language model. Pre-training is the foundational step where the model learns general language patterns from a large corpus of text.\n",
    "\n",
    "## What is Pre-Training?\n",
    "\n",
    "Pre-training teaches a model to predict the next token in a sequence. Given text like:\n",
    "\n",
    "```\n",
    "\"The cat sat on the\" → predict \"mat\"\n",
    "```\n",
    "\n",
    "Through millions of these predictions, the model learns:\n",
    "- Grammar and syntax\n",
    "- Word relationships and semantics  \n",
    "- Facts and knowledge from the training data\n",
    "- Reasoning patterns\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "1. **Model Architecture** - Build all GPT components (attention, feed-forward, transformer blocks)\n",
    "2. **Loss Calculation** - Understand cross-entropy loss for next-token prediction\n",
    "3. **Data Preparation** - Create training batches from raw text\n",
    "4. **Training Loop** - Put it all together to train the model\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Pre-training is **expensive** (GPT-3 cost ~$4.6M to train!) but creates a foundation that can be fine-tuned for many tasks. Understanding this process helps you:\n",
    "- Debug training issues\n",
    "- Make informed architecture decisions\n",
    "- Understand model capabilities and limitations\n",
    "\n",
    "---\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb09ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8ubeyg4p",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "**Layer Normalization** stabilizes training by normalizing activations across the feature dimension.\n",
    "\n",
    "### Why We Need It:\n",
    "\n",
    "During training, the distribution of layer inputs can shift dramatically (called \"internal covariate shift\"). This makes training unstable and slow. Layer normalization fixes this by:\n",
    "\n",
    "1. **Normalizing** each sample independently across features\n",
    "2. **Scaling and shifting** with learnable parameters (so the model can undo normalization if needed)\n",
    "\n",
    "### The Math:\n",
    "\n",
    "```\n",
    "For each token position:\n",
    "    mean = average of all features\n",
    "    var  = variance of all features\n",
    "    \n",
    "    normalized = (x - mean) / √(var + ε)\n",
    "    output = scale * normalized + shift\n",
    "```\n",
    "\n",
    "### Why Layer Norm (not Batch Norm)?\n",
    "\n",
    "| Batch Norm | Layer Norm |\n",
    "|------------|------------|\n",
    "| Normalizes across batch | Normalizes across features |\n",
    "| Depends on batch size | Independent of batch size |\n",
    "| Different behavior train/eval | Same behavior always |\n",
    "| Bad for sequences | Great for sequences |\n",
    "\n",
    "**GPT uses Layer Norm** because it works independently for each token, regardless of batch size or sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea8861d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oggglia1mjl",
   "metadata": {},
   "source": [
    "## GELU Activation Function\n",
    "\n",
    "**GELU** (Gaussian Error Linear Unit) is the activation function used in GPT and most modern transformers.\n",
    "\n",
    "### Why Not ReLU?\n",
    "\n",
    "ReLU is simple (`max(0, x)`) but has a problem: **dead neurons**. Once a neuron outputs 0, it may never recover during training.\n",
    "\n",
    "### How GELU Works:\n",
    "\n",
    "GELU is a **smooth approximation** that:\n",
    "- Allows small negative values through (unlike ReLU which blocks all negatives)\n",
    "- Is differentiable everywhere (unlike ReLU's sharp corner at 0)\n",
    "- Acts like a \"soft gate\" based on the input's value\n",
    "\n",
    "```\n",
    "GELU(x) ≈ 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))\n",
    "```\n",
    "\n",
    "### Why GELU is Better for Transformers:\n",
    "- Smoother gradients → more stable training\n",
    "- No dead neurons → better gradient flow\n",
    "- Empirically works better for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16164a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjvjx8ogw5",
   "metadata": {},
   "source": [
    "## Feed-Forward Network (FFN)\n",
    "\n",
    "The **Feed-Forward Network** is the \"thinking\" part of each transformer block. While attention figures out *what to look at*, the FFN processes *what to do with that information*.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Input [batch, seq, 768]\n",
    "        │\n",
    "        ▼\n",
    "┌───────────────────┐\n",
    "│  Linear (768→3072)│  ← Expand to 4x size\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐\n",
    "│      GELU         │  ← Non-linearity\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐\n",
    "│  Linear (3072→768)│  ← Project back down\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "Output [batch, seq, 768]\n",
    "```\n",
    "\n",
    "### Why 4x Expansion?\n",
    "\n",
    "The FFN temporarily expands the dimension by **4x** (768 → 3072). This gives the network more \"room to think\":\n",
    "\n",
    "1. **More parameters** = more capacity to learn complex patterns\n",
    "2. **Bottleneck design** = forces compression of information\n",
    "3. **Empirically effective** = this ratio works well in practice\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "The FFN processes each token position **independently**. Unlike attention (which mixes information across positions), the FFN applies the same transformation to each token separately. This is why it's also called a \"position-wise\" feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcbc34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mb5qeio24i",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "The **Transformer Block** combines attention and feed-forward layers with **residual connections** and **layer normalization**. This is the repeating unit that gets stacked to create deep models.\n",
    "\n",
    "### Architecture (Pre-Norm variant used in GPT-2/3):\n",
    "\n",
    "```\n",
    "        Input\n",
    "          │\n",
    "          ├─────────────────────┐\n",
    "          │                     │ (residual/skip connection)\n",
    "          ▼                     │\n",
    "    ┌───────────┐               │\n",
    "    │ LayerNorm │               │\n",
    "    └─────┬─────┘               │\n",
    "          │                     │\n",
    "          ▼                     │\n",
    "    ┌───────────┐               │\n",
    "    │ Attention │               │\n",
    "    └─────┬─────┘               │\n",
    "          │                     │\n",
    "          ▼                     │\n",
    "    ┌───────────┐               │\n",
    "    │  Dropout  │               │\n",
    "    └─────┬─────┘               │\n",
    "          │                     │\n",
    "          ▼                     │\n",
    "        (+)←────────────────────┘\n",
    "          │\n",
    "          ├─────────────────────┐\n",
    "          │                     │ (residual/skip connection)\n",
    "          ▼                     │\n",
    "    ┌───────────┐               │\n",
    "    │ LayerNorm │               │\n",
    "    └─────┬─────┘               │\n",
    "          │                     │\n",
    "          ▼                     │\n",
    "    ┌───────────┐               │\n",
    "    │    FFN    │               │\n",
    "    └─────┬─────┘               │\n",
    "          │                     │\n",
    "          ▼                     │\n",
    "    ┌───────────┐               │\n",
    "    │  Dropout  │               │\n",
    "    └─────┬─────┘               │\n",
    "          │                     │\n",
    "          ▼                     │\n",
    "        (+)←────────────────────┘\n",
    "          │\n",
    "        Output\n",
    "```\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "Residual (skip) connections are **critical** for training deep networks:\n",
    "\n",
    "1. **Gradient flow**: Gradients can flow directly through skip connections, avoiding vanishing gradients\n",
    "2. **Identity mapping**: The network can easily learn to \"do nothing\" if that's optimal\n",
    "3. **Incremental learning**: Each layer learns a \"delta\" on top of existing representations\n",
    "\n",
    "### Pre-Norm vs Post-Norm\n",
    "\n",
    "GPT-2/3 use **Pre-Norm** (normalize before attention/FFN), which:\n",
    "- Is more stable during training\n",
    "- Allows for larger learning rates\n",
    "- Enables training very deep models without warmup tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "477a8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5a1p81o7e",
   "metadata": {},
   "source": [
    "## GPT Configuration\n",
    "\n",
    "Before building the model, we define its **hyperparameters**. These control the model's size and capacity.\n",
    "\n",
    "### GPT-2 124M Configuration:\n",
    "\n",
    "| Parameter | Value | Meaning |\n",
    "|-----------|-------|---------|\n",
    "| `vocab_size` | 50,257 | Number of unique tokens (BPE vocabulary) |\n",
    "| `context_length` | 256 | Maximum sequence length (reduced from 1024 for speed) |\n",
    "| `emb_dim` | 768 | Size of token embeddings |\n",
    "| `n_heads` | 12 | Number of attention heads |\n",
    "| `n_layers` | 12 | Number of transformer blocks |\n",
    "| `drop_rate` | 0.1 | Dropout probability (10%) |\n",
    "| `qkv_bias` | False | No bias in Q, K, V projections |\n",
    "\n",
    "### Scaling Laws:\n",
    "\n",
    "The model's capacity roughly scales with:\n",
    "- **Width** (`emb_dim`): More features per token\n",
    "- **Depth** (`n_layers`): More processing steps\n",
    "- **Heads** (`n_heads`): More parallel attention patterns\n",
    "\n",
    "GPT-2 sizes:\n",
    "- **124M**: 12 layers, 768 dim, 12 heads (what we're using)\n",
    "- **355M**: 24 layers, 1024 dim, 16 heads\n",
    "- **774M**: 36 layers, 1280 dim, 20 heads\n",
    "- **1.5B**: 48 layers, 1600 dim, 25 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a24d0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8h3x3h4oyk",
   "metadata": {},
   "source": [
    "## The Complete GPT Model\n",
    "\n",
    "Now we assemble all components into the full **GPTModel** class.\n",
    "\n",
    "### Architecture Overview:\n",
    "\n",
    "```\n",
    "Token IDs [batch, seq_len]\n",
    "          │\n",
    "          ▼\n",
    "┌─────────────────────┐\n",
    "│  Token Embedding    │  Look up vectors for each token\n",
    "│  [vocab → emb_dim]  │\n",
    "└──────────┬──────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌─────────────────────┐\n",
    "│ Position Embedding  │  Add position information\n",
    "│ [seq_len → emb_dim] │\n",
    "└──────────┬──────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌─────────────────────┐\n",
    "│     Dropout         │  Regularization\n",
    "└──────────┬──────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌─────────────────────┐\n",
    "│  Transformer Block  │ ─┐\n",
    "└──────────┬──────────┘  │\n",
    "           │             │\n",
    "           ▼             │ × 12 layers\n",
    "┌─────────────────────┐  │\n",
    "│  Transformer Block  │  │\n",
    "└──────────┬──────────┘  │\n",
    "           │             │\n",
    "          ...           ─┘\n",
    "           │\n",
    "           ▼\n",
    "┌─────────────────────┐\n",
    "│    Final LayerNorm  │\n",
    "└──────────┬──────────┘\n",
    "           │\n",
    "           ▼\n",
    "┌─────────────────────┐\n",
    "│    Output Head      │  Project to vocabulary size\n",
    "│ [emb_dim → vocab]   │\n",
    "└──────────┬──────────┘\n",
    "           │\n",
    "           ▼\n",
    "    Logits [batch, seq_len, vocab_size]\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Token + Position Embeddings**: The model needs both *what* token and *where* it is\n",
    "2. **Stacked Transformer Blocks**: 12 identical blocks, each refining the representations\n",
    "3. **Output Head**: Maps final representations back to vocabulary probabilities\n",
    "4. **No bias in output**: `bias=False` in the final Linear layer (GPT-2 convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e3077ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6q5xqgprjf",
   "metadata": {},
   "source": [
    "## Instantiate the Model\n",
    "\n",
    "Let's create our GPT model and examine its structure.\n",
    "\n",
    "### What Happens Here:\n",
    "\n",
    "1. **Set random seed**: Ensures reproducible weight initialization\n",
    "2. **Create model**: Instantiates all layers with random weights\n",
    "3. **Set to eval mode**: Disables dropout (important for consistent outputs during testing)\n",
    "\n",
    "### Model Size:\n",
    "\n",
    "With our configuration, the model has approximately **124 million parameters**:\n",
    "- Token embeddings: 50,257 × 768 = ~38.6M\n",
    "- Position embeddings: 256 × 768 = ~0.2M  \n",
    "- Each transformer block: ~7M parameters\n",
    "- 12 blocks: ~84M\n",
    "- Output head: 768 × 50,257 = ~38.6M (but shares weights with token embeddings in original GPT-2)\n",
    "\n",
    "**Note**: Our implementation doesn't share weights between token embeddings and output head, so we have slightly more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2b0455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u88tlchooe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Text Generation (Before Training)\n",
    "\n",
    "Before training, let's see what our randomly-initialized model produces. Spoiler: it will be **complete gibberish**!\n",
    "\n",
    "## Text Generation Process\n",
    "\n",
    "The model generates text **one token at a time** using this loop:\n",
    "\n",
    "```\n",
    "1. Input: \"Every effort moves you\"\n",
    "2. Model predicts probability distribution over ALL 50,257 tokens\n",
    "3. Select the most likely token (argmax) or sample from distribution\n",
    "4. Append new token to input\n",
    "5. Repeat steps 2-4 for desired length\n",
    "```\n",
    "\n",
    "### Helper Functions:\n",
    "\n",
    "- **`text_to_token_ids`**: Converts text → token IDs using GPT-2's BPE tokenizer\n",
    "- **`token_ids_to_text`**: Converts token IDs → text\n",
    "- **`generate_text_simple`**: The generation loop (greedy decoding with argmax)\n",
    "\n",
    "### Why the Output is Garbage:\n",
    "\n",
    "Our model has **random weights**. It hasn't learned:\n",
    "- What words mean\n",
    "- Grammar rules\n",
    "- Which tokens typically follow others\n",
    "\n",
    "The output will be essentially random tokens. This demonstrates why **pre-training is essential**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fb0f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def generate_text_simple(model, idx,\n",
    "                         max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qj9jyhq1wi",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Understanding the Loss Function\n",
    "\n",
    "Now we dive into the **most important concept for training**: the loss function.\n",
    "\n",
    "## What is Loss?\n",
    "\n",
    "**Loss** measures how wrong our model's predictions are. For language models:\n",
    "- Lower loss = better predictions\n",
    "- Higher loss = worse predictions\n",
    "\n",
    "## Cross-Entropy Loss for Language Modeling\n",
    "\n",
    "For next-token prediction, we use **cross-entropy loss**:\n",
    "\n",
    "```\n",
    "Loss = -log(probability of correct token)\n",
    "```\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "If the model predicts:\n",
    "- Correct token with 90% probability → Loss = -log(0.9) = 0.105 (low, good!)\n",
    "- Correct token with 1% probability → Loss = -log(0.01) = 4.6 (high, bad!)\n",
    "- Correct token with 0.01% probability → Loss = -log(0.0001) = 9.2 (very high, terrible!)\n",
    "\n",
    "## Setting Up the Example\n",
    "\n",
    "We'll use two short sequences to understand how loss is calculated:\n",
    "\n",
    "```\n",
    "Batch 1: \"every effort moves\" → target: \"effort moves you\"\n",
    "Batch 2: \"I really like\"      → target: \"really like chocolate\"\n",
    "```\n",
    "\n",
    "Each input token should predict the **next** token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9804f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1wy4t1tzrg5",
   "metadata": {},
   "source": [
    "### Create Input Tokens\n",
    "\n",
    "These are the **token IDs** for our input sequences. The tokenizer converts words to numbers:\n",
    "- `16833` = \"every\"\n",
    "- `3626` = \" effort\" \n",
    "- `6100` = \" moves\"\n",
    "- `40` = \"I\"\n",
    "- `1107` = \" really\"\n",
    "- `588` = \" like\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9eb5c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tzsrwh3i3o",
   "metadata": {},
   "source": [
    "### Create Target Tokens\n",
    "\n",
    "The targets are simply the inputs **shifted by one position**. This is the essence of language modeling:\n",
    "\n",
    "```\n",
    "Input:  [every]  [effort] [moves]\n",
    "Target: [effort] [moves]  [you]\n",
    "         ↑        ↑        ↑\n",
    "      predict   predict  predict\n",
    "```\n",
    "\n",
    "Each position learns to predict the **next** token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09ebe92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3pfdhoqjz",
   "metadata": {},
   "source": [
    "### Get Model Predictions\n",
    "\n",
    "Run the inputs through the model to get **logits** (raw scores before softmax).\n",
    "\n",
    "The output shape is `[batch_size, seq_len, vocab_size]` = `[2, 3, 50257]`:\n",
    "- 2 sequences in the batch\n",
    "- 3 tokens per sequence\n",
    "- 50,257 scores (one for each possible next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08fbb0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w9j1f0nv3ao",
   "metadata": {},
   "source": [
    "### What Would the Model Predict?\n",
    "\n",
    "Let's see which tokens the model would actually generate (using argmax to pick the highest probability token).\n",
    "\n",
    "**Remember**: The model has random weights, so these predictions will be nonsense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8d7bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j895wxrbkqp",
   "metadata": {},
   "source": [
    "### Compare Predictions vs Targets\n",
    "\n",
    "Let's decode both the targets (what we want) and predictions (what the model says).\n",
    "\n",
    "As expected, the predictions are **completely wrong** - random tokens that don't make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0ba4536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdgoffb7c5",
   "metadata": {},
   "source": [
    "### Extract Probabilities for Target Tokens\n",
    "\n",
    "Now let's see **how much probability** the model assigned to the correct tokens.\n",
    "\n",
    "We use fancy indexing to extract just the probabilities for our target tokens:\n",
    "- `probas[0, [0,1,2], targets[0]]` = probabilities at positions 0,1,2 for the target token IDs\n",
    "\n",
    "**Expected**: Very low probabilities (around 1/50,257 ≈ 0.00002) since the model is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71d4f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02jjqkjgaqtd",
   "metadata": {},
   "source": [
    "### Convert to Log Probabilities\n",
    "\n",
    "We take the **logarithm** of probabilities because:\n",
    "\n",
    "1. **Numerical stability**: Very small probabilities (like 0.00001) become manageable numbers\n",
    "2. **Easier math**: Products become sums (important for sequences)\n",
    "3. **Convention**: Cross-entropy loss uses log probabilities\n",
    "\n",
    "```\n",
    "prob = 0.00007  →  log(prob) = -9.5\n",
    "prob = 0.00001  →  log(prob) = -11.5\n",
    "```\n",
    "\n",
    "**Note**: More negative = lower probability = worse prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d13e69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qb3r1hjk7a",
   "metadata": {},
   "source": [
    "### Average Log Probability\n",
    "\n",
    "We average the log probabilities across all tokens to get a single number representing overall model performance.\n",
    "\n",
    "This average is **negative** because log probabilities are always ≤ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "139fe59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zr7q8931jb",
   "metadata": {},
   "source": [
    "### Negative Average Log Probability = Cross-Entropy Loss!\n",
    "\n",
    "We negate the average to get a **positive** loss value:\n",
    "\n",
    "```\n",
    "Cross-Entropy Loss = -mean(log(probability of correct tokens))\n",
    "```\n",
    "\n",
    "This is exactly what `torch.nn.functional.cross_entropy` computes!\n",
    "\n",
    "**Goal of training**: Minimize this loss → Maximize probability of correct tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62f8d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7nt01tkoeyh",
   "metadata": {},
   "source": [
    "## Using PyTorch's Cross-Entropy Function\n",
    "\n",
    "The manual calculation above was educational, but in practice we use PyTorch's optimized `cross_entropy` function.\n",
    "\n",
    "### The Challenge: Shape Mismatch\n",
    "\n",
    "PyTorch's `cross_entropy` expects:\n",
    "- `input`: `[N, C]` where N = samples, C = classes\n",
    "- `target`: `[N]` \n",
    "\n",
    "But our shapes are:\n",
    "- `logits`: `[batch, seq_len, vocab]` = `[2, 3, 50257]`\n",
    "- `targets`: `[batch, seq_len]` = `[2, 3]`\n",
    "\n",
    "**Solution**: Flatten the batch and sequence dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5fbb4570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zw5ndm4ak8",
   "metadata": {},
   "source": [
    "### Flatten for Cross-Entropy\n",
    "\n",
    "```\n",
    "Before flattening:\n",
    "  logits:  [2, 3, 50257]  →  2 batches × 3 positions × 50257 vocab\n",
    "  targets: [2, 3]         →  2 batches × 3 positions\n",
    "\n",
    "After flattening:\n",
    "  logits:  [6, 50257]     →  6 total predictions × 50257 vocab\n",
    "  targets: [6]            →  6 total targets\n",
    "```\n",
    "\n",
    "Now each of the 6 predictions is treated as an independent classification problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d324b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9mrs9vqbs",
   "metadata": {},
   "source": [
    "### Compute Cross-Entropy Loss\n",
    "\n",
    "PyTorch's `cross_entropy`:\n",
    "1. Applies softmax to logits internally\n",
    "2. Takes log\n",
    "3. Selects values at target indices\n",
    "4. Averages and negates\n",
    "\n",
    "The result matches our manual calculation exactly (10.7940)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab58cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3rdr3u7x2j",
   "metadata": {},
   "source": [
    "## Perplexity: A More Interpretable Metric\n",
    "\n",
    "**Perplexity** is cross-entropy loss converted to a more intuitive scale:\n",
    "\n",
    "```\n",
    "Perplexity = e^(cross_entropy_loss)\n",
    "```\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "Perplexity ≈ \"How many tokens is the model confused between?\"\n",
    "\n",
    "| Perplexity | Meaning |\n",
    "|------------|---------|\n",
    "| 1 | Perfect! Model is 100% confident and correct |\n",
    "| 10 | Model is \"choosing\" between ~10 equally likely tokens |\n",
    "| 100 | Model is \"choosing\" between ~100 equally likely tokens |\n",
    "| 50,000 | Nearly random guessing (vocab size is 50,257) |\n",
    "\n",
    "### For Our Random Model:\n",
    "\n",
    "Perplexity ≈ 48,726 means the model is essentially guessing randomly among all tokens. After training, we'd expect perplexity in the range of 10-50 for a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff0954ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'pg84.txt'...\n",
      "Successfully downloaded 'pg84.txt'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import requests\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
    "filename = \"pg84.txt\"\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Downloading '{filename}'...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Successfully downloaded '{filename}'\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(f\"'{filename}' already exists, skipping download.\")\n",
    "\n",
    "# Load the text\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nvwgq31ucz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Preparing Training Data\n",
    "\n",
    "Now we need real text data to train on. We'll use **\"Frankenstein\" by Mary Shelley** from Project Gutenberg - it's free, classic literature, and a good size for demonstration.\n",
    "\n",
    "## The Data Pipeline:\n",
    "\n",
    "```\n",
    "Raw Text File\n",
    "     │\n",
    "     ▼\n",
    "┌─────────────┐\n",
    "│  Tokenize   │  Convert text → token IDs using BPE\n",
    "└──────┬──────┘\n",
    "       │\n",
    "       ▼\n",
    "┌─────────────┐\n",
    "│   Create    │  Sliding window to create input-target pairs\n",
    "│   Dataset   │\n",
    "└──────┬──────┘\n",
    "       │\n",
    "       ▼\n",
    "┌─────────────┐\n",
    "│ DataLoader  │  Batch, shuffle, and iterate efficiently\n",
    "└──────┬──────┘\n",
    "       │\n",
    "       ▼\n",
    "  Training Loop\n",
    "```\n",
    "\n",
    "## Download the Training Text\n",
    "\n",
    "We'll download Frankenstein (~440KB of text, ~106K tokens) - small enough to train quickly but large enough to see real learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09ed44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 438807\n",
      "Tokens: 106361\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wxqwebjaq",
   "metadata": {},
   "source": [
    "### Check Dataset Size\n",
    "\n",
    "Let's see how much data we have:\n",
    "- **Characters**: Raw text length\n",
    "- **Tokens**: After BPE tokenization (typically 3-4 characters per token for English)\n",
    "\n",
    "~106K tokens is small by modern standards (GPT-3 trained on 300B tokens!) but sufficient for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd6e710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bks879rtfh",
   "metadata": {},
   "source": [
    "### Train/Validation Split\n",
    "\n",
    "We split the data into:\n",
    "- **90% Training**: Model learns from this\n",
    "- **10% Validation**: Used to check if model is overfitting\n",
    "\n",
    "**Important**: We split by position, not randomly. This ensures:\n",
    "1. No data leakage between train and validation\n",
    "2. Validation tests the model on \"future\" text it hasn't seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "hdeeeomawj",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    \"\"\"Creates input-target pairs using sliding window for next-token prediction.\"\"\"\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \"\"\"Create a DataLoader with GPT-2 BPE tokenization.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wpfpmxk15g",
   "metadata": {},
   "source": [
    "## GPT Dataset: Creating Input-Target Pairs\n",
    "\n",
    "The `GPTDatasetV1` class creates training examples using a **sliding window**:\n",
    "\n",
    "```\n",
    "Text: \"The monster approached the village slowly\"\n",
    "      ─────────────────────────────────────────\n",
    "\n",
    "With max_length=4, stride=4:\n",
    "\n",
    "Example 1:\n",
    "  Input:  [The, monster, approached, the]\n",
    "  Target: [monster, approached, the, village]\n",
    "  \n",
    "Example 2:\n",
    "  Input:  [village, slowly, ...]\n",
    "  Target: [slowly, ..., ...]\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Effect |\n",
    "|-----------|--------|\n",
    "| `max_length` | Sequence length (context window size) |\n",
    "| `stride` | How far to move between examples |\n",
    "\n",
    "### Stride Strategies:\n",
    "\n",
    "- **stride = max_length**: No overlap, each token used once per epoch\n",
    "- **stride < max_length**: Overlapping windows, more examples but redundant data\n",
    "- **stride = 1**: Maximum examples, but highly redundant\n",
    "\n",
    "We use `stride = max_length` (256) for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52f89697",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hujsejvwmqo",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "**DataLoaders** handle:\n",
    "- **Batching**: Group examples together for parallel processing\n",
    "- **Shuffling**: Randomize order each epoch (training only)\n",
    "- **Dropping incomplete batches**: Ensures consistent batch sizes\n",
    "\n",
    "Settings:\n",
    "- `batch_size=2`: Small for demonstration (real training uses 32-512)\n",
    "- `shuffle=True` for training, `False` for validation\n",
    "- `drop_last=True` for training to avoid small final batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2298bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ulyhm9iqyt",
   "metadata": {},
   "source": [
    "### Verify Data Loader Output\n",
    "\n",
    "Let's check that our data loaders produce the expected shapes:\n",
    "- Each batch should have shape `[batch_size, seq_length]` = `[2, 256]`\n",
    "- Both inputs (x) and targets (y) have the same shape\n",
    "\n",
    "The training loader should have more batches than the validation loader (90% vs 10% of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2851369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kbhfzhdlx9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Training Infrastructure\n",
    "\n",
    "Before the main training loop, we need helper functions to calculate loss efficiently.\n",
    "\n",
    "## Batch Loss Function\n",
    "\n",
    "`calc_loss_batch` computes the cross-entropy loss for a single batch:\n",
    "\n",
    "1. Move data to the right device (CPU or GPU)\n",
    "2. Run forward pass to get logits\n",
    "3. Flatten and compute cross-entropy loss\n",
    "\n",
    "This is the same loss calculation we did manually earlier, but packaged as a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd852dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atsywcg3dwk",
   "metadata": {},
   "source": [
    "## Dataset Loss Function\n",
    "\n",
    "`calc_loss_loader` computes the average loss over an entire data loader:\n",
    "\n",
    "1. Iterate through batches\n",
    "2. Accumulate losses\n",
    "3. Return the average\n",
    "\n",
    "### Optional `num_batches` Parameter:\n",
    "\n",
    "For large datasets, computing loss on ALL batches is slow. The `num_batches` parameter lets you estimate the loss using only a subset of data - useful for quick progress checks during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0c3dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "co41rtlgkfv",
   "metadata": {},
   "source": [
    "## Device Selection\n",
    "\n",
    "Deep learning benefits enormously from **GPU acceleration**:\n",
    "\n",
    "| Device | Training Speed |\n",
    "|--------|---------------|\n",
    "| CPU | 1x (baseline) |\n",
    "| GPU (CUDA) | 10-100x faster |\n",
    "| TPU | Even faster (specialized) |\n",
    "\n",
    "PyTorch automatically detects if a CUDA-capable GPU is available. If not, it falls back to CPU.\n",
    "\n",
    "**Note**: Training on CPU is fine for learning, but real models require GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed068146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.986330868726109\n",
      "Validation loss: 10.984492619832357\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pv1aqguesck",
   "metadata": {},
   "source": [
    "## Baseline Loss (Before Training)\n",
    "\n",
    "Let's measure the loss on both training and validation sets **before any training**.\n",
    "\n",
    "### What to Expect:\n",
    "\n",
    "For a randomly initialized model with vocabulary size 50,257:\n",
    "- Expected loss ≈ log(50,257) ≈ **10.82**\n",
    "- This is the loss you'd get from random guessing\n",
    "\n",
    "If our initial loss is close to 10.82, it confirms the model is starting from random weights (as expected).\n",
    "\n",
    "### Why Check Both Train and Val Loss?\n",
    "\n",
    "- **Training loss**: How well does the model fit the training data?\n",
    "- **Validation loss**: How well does the model generalize to unseen data?\n",
    "\n",
    "If training loss goes down but validation loss goes up → **overfitting**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
