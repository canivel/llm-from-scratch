{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1z0rik3wlc3",
   "metadata": {},
   "source": [
    "# Chapter 7: The Transformer Block\n",
    "\n",
    "## Bringing It All Together\n",
    "\n",
    "In the previous notebooks, we built each component individually:\n",
    "- **Causal Attention** - How tokens look at previous tokens\n",
    "- **Multi-Head Attention** - Multiple parallel attention patterns  \n",
    "- **Layer Normalization** - Stabilizing activations\n",
    "- **GELU Activation** - Smooth non-linearity\n",
    "- **Feed-Forward Network** - Processing each position\n",
    "\n",
    "Now it's time to **assemble these building blocks** into a complete **Transformer Block** - the fundamental unit that gets stacked to create GPT!\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    TRANSFORMER BLOCK                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚    Input â”€â”€â”¬â”€â”€â–º LayerNorm â”€â”€â–º Multi-Head â”€â”€â–º Dropout â”€â”€â”   â”‚\n",
    "â”‚            â”‚                  Attention                 â”‚   â”‚\n",
    "â”‚            â”‚                                            â”‚   â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                â”‚                            â”‚\n",
    "â”‚                                â–¼                            â”‚\n",
    "â”‚            â”Œâ”€â”€â–º LayerNorm â”€â”€â–º FeedForward â”€â”€â–º Dropout â”€â”€â”   â”‚\n",
    "â”‚            â”‚                                             â”‚   â”‚\n",
    "â”‚            â”‚                                             â”‚   â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                â”‚                            â”‚\n",
    "â”‚                                â–¼                            â”‚\n",
    "â”‚                             Output                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Key Insight: The \"Pre-LN\" Architecture\n",
    "\n",
    "GPT-2 uses **Pre-Layer Normalization** (Pre-LN), where LayerNorm comes *before* each sub-layer:\n",
    "\n",
    "| Architecture | Order | Advantage |\n",
    "|-------------|-------|-----------|\n",
    "| **Post-LN** (Original) | Attention â†’ Add â†’ Norm | Harder to train deep networks |\n",
    "| **Pre-LN** (GPT-2) | Norm â†’ Attention â†’ Add | More stable training |\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d22ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ynvp2fek2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Block 1: Multi-Head Attention (Recap)\n",
    "\n",
    "This is the same Multi-Head Attention we built in Chapter 5. Let's recall what it does:\n",
    "\n",
    "### What Multi-Head Attention Computes\n",
    "\n",
    "Each **head** learns to focus on different aspects of the input:\n",
    "- Head 1 might focus on **syntax** (subject-verb relationships)\n",
    "- Head 2 might focus on **semantics** (word meaning connections)\n",
    "- Head 3 might focus on **position** (nearby words)\n",
    "\n",
    "### The Efficient Implementation\n",
    "\n",
    "Instead of running `num_heads` separate attention modules, we:\n",
    "\n",
    "1. **Project once** with large matrices (d_in â†’ d_out)\n",
    "2. **Reshape** to split into heads: `(batch, seq, d_out)` â†’ `(batch, seq, num_heads, head_dim)`\n",
    "3. **Transpose** to get heads dimension first: `(batch, num_heads, seq, head_dim)`\n",
    "4. **Compute attention** for all heads in parallel\n",
    "5. **Reshape back** and project output\n",
    "\n",
    "```\n",
    "Input: (batch=2, tokens=4, d_in=768)\n",
    "                    â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â–¼           â–¼           â–¼\n",
    "     W_query     W_key      W_value\n",
    "        â”‚           â”‚           â”‚\n",
    "        â–¼           â–¼           â–¼\n",
    "   (2,4,768)   (2,4,768)   (2,4,768)\n",
    "        â”‚           â”‚           â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚ reshape to 12 heads\n",
    "                    â–¼\n",
    "            (2, 12, 4, 64)  â—„â”€â”€ 768 Ã· 12 = 64 per head\n",
    "                    â”‚\n",
    "              Attention\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "            (2, 12, 4, 64)\n",
    "                    â”‚ reshape back\n",
    "                    â–¼\n",
    "              (2, 4, 768)\n",
    "                    â”‚\n",
    "               out_proj\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "         Output: (2, 4, 768)\n",
    "```\n",
    "\n",
    "### Key Features in This Implementation\n",
    "\n",
    "| Feature | Purpose |\n",
    "|---------|---------|\n",
    "| `qkv_bias=False` | GPT-2 doesn't use bias in Q, K, V projections |\n",
    "| Causal mask | Prevents attending to future tokens |\n",
    "| Dropout | Regularization on attention weights |\n",
    "| `out_proj` | Final linear projection to mix head outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb09ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exnw60x0jbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Block 2: Layer Normalization (Recap)\n",
    "\n",
    "Layer Normalization is crucial for **stable training** of deep transformers.\n",
    "\n",
    "### Why We Need Normalization\n",
    "\n",
    "As data flows through many layers, values can:\n",
    "- **Explode** â†’ gradients become NaN\n",
    "- **Vanish** â†’ gradients become 0, no learning\n",
    "\n",
    "LayerNorm keeps values in a stable range after each sub-layer.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "For each token independently, normalize across the embedding dimension:\n",
    "\n",
    "```\n",
    "                    Token embedding (768 values)\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ 2.1  -0.3  1.5  ...  0.8   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ Compute mean (Î¼)   â”‚ â†’ single value\n",
    "                    â”‚ Compute var (ÏƒÂ²)   â”‚ â†’ single value  \n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  (x - Î¼)          â”‚\n",
    "                    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚  Normalize\n",
    "                    â”‚  âˆš(ÏƒÂ² + Îµ)        â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ scale * x + shift â”‚  Learnable params\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ 0.5  -0.1  0.3  ...  0.2     â”‚  Normalized\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Learnable Parameters\n",
    "\n",
    "| Parameter | Shape | Initial Value | Purpose |\n",
    "|-----------|-------|---------------|---------|\n",
    "| `scale` | (768,) | 1.0 | Allow network to \"undo\" normalization if needed |\n",
    "| `shift` | (768,) | 0.0 | Allow shifting the mean |\n",
    "\n",
    "These let the network learn the optimal distribution for each feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8861d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fkbr8vzmk7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Block 3: GELU Activation (Recap)\n",
    "\n",
    "**GELU** (Gaussian Error Linear Unit) is the activation function used in GPT-2, BERT, and most modern transformers.\n",
    "\n",
    "### Why Not ReLU?\n",
    "\n",
    "| Activation | Formula | Problem |\n",
    "|------------|---------|---------|\n",
    "| **ReLU** | max(0, x) | \"Dead neurons\" - once output is 0, gradient is 0 forever |\n",
    "| **GELU** | x Â· Î¦(x) | Smooth, no dead neurons, probabilistic interpretation |\n",
    "\n",
    "\n",
    "### The GELU Formula\n",
    "\n",
    "GELU can be approximated as:\n",
    "\n",
    "```\n",
    "GELU(x) â‰ˆ 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))\n",
    "```\n",
    "\n",
    "This smooth curve:\n",
    "- Acts like identity for large positive x\n",
    "- Acts like 0 for large negative x  \n",
    "- Allows **small gradients** for small negative values (unlike ReLU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16164a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8yie4gn0y",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Block 4: Feed-Forward Network (Recap)\n",
    "\n",
    "The Feed-Forward Network (FFN) is a simple but powerful component applied to **each token position independently**.\n",
    "\n",
    "### The \"Expand then Contract\" Pattern\n",
    "\n",
    "```\n",
    "    Token embedding                    Token embedding\n",
    "       (768)                              (768)\n",
    "         â”‚                                  â–²\n",
    "         â–¼                                  â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Linear  â”‚  768 â†’ 3072            â”‚ Linear  â”‚  3072 â†’ 768\n",
    "    â”‚  (4x)   â”‚                        â”‚  (Ã·4)   â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "         â”‚                                  â”‚\n",
    "         â–¼                                  â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\n",
    "    â”‚  GELU   â”‚  Non-linearity              â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                             â”‚\n",
    "         â”‚                                  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why Expand 4x?\n",
    "\n",
    "The expansion factor of 4 is a **design choice** that works well empirically:\n",
    "\n",
    "| Layer | Dimensions | Parameters |\n",
    "|-------|-----------|------------|\n",
    "| Linear 1 | 768 â†’ 3072 | 768 Ã— 3072 = 2.4M |\n",
    "| Linear 2 | 3072 â†’ 768 | 3072 Ã— 768 = 2.4M |\n",
    "| **Total** | | **4.8M per block** |\n",
    "\n",
    "The larger intermediate dimension allows the network to:\n",
    "- Learn more complex transformations\n",
    "- Store more \"knowledge\" in the weights\n",
    "- Act as a **key-value memory** (recent research insight!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbc34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uoh5at3fv9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ The Main Event: TransformerBlock\n",
    "\n",
    "Now we combine all four building blocks into a single **TransformerBlock**!\n",
    "\n",
    "### The Architecture (Pre-LN Style)\n",
    "\n",
    "```\n",
    "                         Input x\n",
    "                            â”‚\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â”‚                â”‚                â”‚\n",
    "           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "           â”‚         â”‚  LayerNorm  â”‚         â”‚\n",
    "           â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "           â”‚                â”‚                â”‚\n",
    "           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "           â”‚         â”‚   Multi-    â”‚         â”‚\n",
    "           â”‚         â”‚    Head     â”‚         â”‚\n",
    "           â”‚         â”‚  Attention  â”‚         â”‚\n",
    "           â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "           â”‚                â”‚                â”‚\n",
    "           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "           â”‚         â”‚   Dropout   â”‚         â”‚\n",
    "           â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "           â”‚                â”‚                â”‚\n",
    "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º (+) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚ â—„â”€â”€ Residual Connection 1\n",
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â”‚                â”‚                â”‚\n",
    "           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "           â”‚         â”‚  LayerNorm  â”‚         â”‚\n",
    "           â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "           â”‚                â”‚                â”‚\n",
    "           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "           â”‚         â”‚ FeedForward â”‚         â”‚\n",
    "           â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "           â”‚                â”‚                â”‚\n",
    "           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "           â”‚         â”‚   Dropout   â”‚         â”‚\n",
    "           â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "           â”‚                â”‚                â”‚\n",
    "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º (+) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚ â—„â”€â”€ Residual Connection 2\n",
    "                            â–¼\n",
    "                         Output\n",
    "```\n",
    "\n",
    "### Understanding the Code\n",
    "\n",
    "Let's break down the `forward` method step by step:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PART 1: Attention Sub-Block with Residual\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    shortcut = x              # Save input for residual\n",
    "    x = self.norm1(x)         # Pre-LayerNorm (normalize first!)\n",
    "    x = self.att(x)           # Multi-Head Attention\n",
    "    x = self.drop_shortcut(x) # Dropout for regularization\n",
    "    x = x + shortcut          # Add residual (skip connection)\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PART 2: FeedForward Sub-Block with Residual  \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    shortcut = x              # Save for second residual\n",
    "    x = self.norm2(x)         # Pre-LayerNorm\n",
    "    x = self.ff(x)            # FeedForward network\n",
    "    x = self.drop_shortcut(x) # Dropout\n",
    "    x = x + shortcut          # Add residual\n",
    "    \n",
    "    return x\n",
    "```\n",
    "\n",
    "### Why Residual Connections Are Critical\n",
    "\n",
    "Without residual connections, training 12+ layer transformers is nearly impossible:\n",
    "\n",
    "| Depth | Without Residual | With Residual |\n",
    "|-------|-----------------|---------------|\n",
    "| 6 layers | Trainable | Trainable |\n",
    "| 12 layers | Vanishing gradients | Trainable âœ“ |\n",
    "| 24 layers | Won't converge | Trainable âœ“ |\n",
    "| 96 layers | Impossible | Trainable âœ“ |\n",
    "\n",
    "The residual connection creates a \"gradient highway\" that allows gradients to flow directly backward through the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477a8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpprh657cg",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GPT-2 Configuration (124M Parameters)\n",
    "\n",
    "Before we can instantiate our TransformerBlock, we need to define the configuration. This is the same config we'll use for the full GPT model:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `vocab_size` | 50,257 | Number of unique tokens (BPE vocabulary) |\n",
    "| `context_length` | 1,024 | Maximum sequence length |\n",
    "| `emb_dim` | 768 | Embedding dimension (d_model) |\n",
    "| `n_heads` | 12 | Number of attention heads |\n",
    "| `n_layers` | 12 | Number of transformer blocks |\n",
    "| `drop_rate` | 0.1 | Dropout probability (10%) |\n",
    "| `qkv_bias` | False | No bias in Q, K, V projections |\n",
    "\n",
    "### Parameter Count for One TransformerBlock\n",
    "\n",
    "Let's calculate how many parameters are in a single block:\n",
    "\n",
    "```\n",
    "Multi-Head Attention:\n",
    "  - W_query:  768 Ã— 768 = 590,000\n",
    "  - W_key:    768 Ã— 768 = 590,000\n",
    "  - W_value:  768 Ã— 768 = 590,000\n",
    "  - out_proj: 768 Ã— 768 = 590,000\n",
    "  Subtotal: ~2.4M\n",
    "\n",
    "FeedForward:\n",
    "  - Linear1: 768 Ã— 3072 = 2,360,000\n",
    "  - Linear2: 3072 Ã— 768 = 2,360,000\n",
    "  Subtotal: ~4.7M\n",
    "\n",
    "LayerNorm (Ã—2):\n",
    "  - scale + shift: 768 Ã— 2 Ã— 2 = 3,000\n",
    "\n",
    "Total per block: ~7.1M parameters\n",
    "Total for 12 blocks: ~85M (+ embeddings = 124M)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a24d0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6w1pu9q0z8l",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing Our TransformerBlock\n",
    "\n",
    "Let's verify that our TransformerBlock works correctly by passing through a random tensor:\n",
    "\n",
    "- **Input**: Random tensor of shape `(batch=2, tokens=4, emb_dim=768)`\n",
    "- **Expected output**: Same shape `(2, 4, 768)`\n",
    "\n",
    "This confirms that the block preserves dimensions - it transforms the representations without changing their shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e51563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u7v978nwug",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "We combined all our building blocks into a **TransformerBlock** - the fundamental repeating unit of GPT:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    TransformerBlock                        â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚   â”‚LayerNorm â”‚ â†’ â”‚   MHA    â”‚ â†’ â”‚ Dropout  â”‚ â†’ â”‚   +    â”‚ â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚        â”‚                                           â–²       â”‚\n",
    "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Residual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚   â”‚LayerNorm â”‚ â†’ â”‚   FFN    â”‚ â†’ â”‚ Dropout  â”‚ â†’ â”‚   +    â”‚ â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚        â”‚                                           â–²       â”‚\n",
    "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Residual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Pre-LN Architecture**: LayerNorm comes *before* each sub-layer (not after)\n",
    "   - This is what GPT-2 uses\n",
    "   - Makes training more stable\n",
    "\n",
    "2. **Two Sub-Blocks**: Each TransformerBlock has:\n",
    "   - **Attention sub-block**: Allows tokens to communicate\n",
    "   - **FeedForward sub-block**: Processes each token independently\n",
    "\n",
    "3. **Residual Connections**: \n",
    "   - Add the input directly to the output\n",
    "   - Creates \"gradient highways\" for deep networks\n",
    "   - Essential for training 12+ layer models\n",
    "\n",
    "4. **Shape Preservation**: Input and output have the same shape\n",
    "   - This allows stacking multiple blocks!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next chapter, we'll stack 12 of these TransformerBlocks together with:\n",
    "- Token embeddings\n",
    "- Position embeddings  \n",
    "- Final LayerNorm\n",
    "- Output projection (language model head)\n",
    "\n",
    "...to create the complete **GPT-2 model**! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
